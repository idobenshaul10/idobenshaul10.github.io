<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xml" href="http://localhost:4000/feed.xslt.xml"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-07-25T17:57:44+03:00</updated><id>http://localhost:4000//</id><title type="html">Ido Ben-Shaul</title><subtitle>PhD student at Tel Aviv University</subtitle><entry><title type="html">Sample blog post</title><link href="http://localhost:4000/sample_blog" rel="alternate" type="text/html" title="Sample blog post" /><published>2021-05-05T23:10:33+03:00</published><updated>2021-05-05T23:10:33+03:00</updated><id>http://localhost:4000/how-to-add-blog</id><content type="html" xml:base="http://localhost:4000/sample_blog">&lt;!--more--&gt;
&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;this-is-my-first-post&quot;&gt;This is my first post&lt;/h2&gt;
&lt;p&gt;yad yday ydad
&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;&lt;/p&gt;</content><summary type="html"></summary></entry><entry><title type="html">Exploring Explainability for Vision Transformers</title><link href="http://localhost:4000/deeplearning/vision-transformer-explainability" rel="alternate" type="text/html" title="Exploring Explainability for Vision Transformers" /><published>2020-12-31T22:10:33+02:00</published><updated>2020-12-31T22:10:33+02:00</updated><id>http://localhost:4000/deeplearning/exploring-explainability-for-vision-transformers</id><content type="html" xml:base="http://localhost:4000/deeplearning/vision-transformer-explainability">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#q--k--v-and-attention&quot;&gt;Q, K, V and Attention.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#visual-examples-of-k-and-q---different-patterns-of-information-flowing&quot;&gt;Visual Examples of K and Q - different patterns of information flowing&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#pattern-1---the-information-flows-in-one-direction&quot;&gt;Pattern 1 - The information flows in one direction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pattern-2---the-information-flows-in-two-directions&quot;&gt;Pattern 2 - The information flows in two directions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#how-do-the-attention-activations-look-like-for-the-class-token-throughout-the-network-&quot;&gt;How do the Attention Activations look like for the class token throughout the network?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#attention-rollout&quot;&gt;Attention Rollout&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#we-have-multiple-attention-heads-what-do-we-do-about-them-&quot;&gt;We have multiple attention heads. What do we do about them?&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#modifications-to-get-attention-rollout-working-with-vision-transformers&quot;&gt;Modifications to get Attention Rollout working with Vision Transformers&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#the-way-we-fuse-the-attention-heads-matters&quot;&gt;The way we fuse the attention heads matters&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#we-can-focus-only-on-the-top-attentions--and-discard-the-rest&quot;&gt;We can focus only on the top attentions, and discard the rest&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gradient-attention-rollout-for-class-specific-explainability&quot;&gt;Gradient Attention Rollout for Class Specific Explainability&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#where-does-the-transformer-see-a-dog--category-243---and-a-cat--category-282--&quot;&gt;Where does the Transformer see a Dog (category 243), and a Cat (category 282)?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#where-does-the-transformer-see-a-musket-dog--category-161--and-a-parrot--category-87--&quot;&gt;Where does the Transformer see a Musket dog (category 161) and a Parrot (category 87)?&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-activation-maximization-tells-us&quot;&gt;What Activation Maximization Tells us&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;In the last few months before writing this post, there seems to be a sort of a breakthrough in bringing Transformers into the world of Computer Vision.&lt;/p&gt;

&lt;p&gt;To list a few notable works about this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.12877&quot;&gt;Training data-efficient image transformers &amp;amp; distillation through attention&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If I can make a prediction for 2021 - in the next year we are going to see &lt;strong&gt;A LOT&lt;/strong&gt; of papers about using Transformers in vision tasks (feel free to comment here in one year if I’m wrong).&lt;/p&gt;

&lt;p&gt;But what is going on inside Vision Transformers? How do they even work? Can we poke at them and dissect them into pieces to understand them better?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;“Explainability” might be an ambitious and over-loaded term that means different things to different people, but when I say Explainability I mean the next things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;(useful for the developer)&lt;/em&gt; &lt;strong&gt;What’s going on inside when we run the Transformer on this image?&lt;/strong&gt; 
Being able to look at intermediate activation layers.
In computer vision - these are usually images!
These are kind of interpretable since you can display the different channel activations as 2D images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;(useful for the developer&lt;/em&gt;) &lt;strong&gt;What did it learn?&lt;/strong&gt;
Being able to investigate what kind of patterns (if any) did the model learn.
Usually this is in the form of the question “What input image maximizes the response from this activation?” , and you can use variants of “Activation Maximization” for that.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;(useful for both the developer and the user)&lt;/em&gt; &lt;strong&gt;What did it see in this image?&lt;/strong&gt;
Being able to Answer “What part of the image is responsible for the network prediction”, sometimes called “Pixel Attribution”.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;So we are going to need this for Vision Transformers as well!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this post we will go over my attempt to do this for Vision Transformers.&lt;/p&gt;

&lt;p&gt;Everything here is going to be done with the recently released ‘Deit Tiny’ model from Facebook, i.e:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'facebookresearch/deit:main'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'deit_tiny_patch16_224'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And we are going to assume 224xx224 input images to make it easier follow the shapes, although it doesn’t have to be.&lt;/p&gt;

&lt;p&gt;Python code is released here: https://github.com/jacobgil/vit-explain&lt;/p&gt;

&lt;p&gt;The rest of this post assumes you understand how Vision Transformers work.&lt;/p&gt;

&lt;p&gt;They are basically vanilla transformers, but the images are split into 14x14 different tokens, where every token represents a 16x16 patch from the image.&lt;/p&gt;

&lt;p&gt;Before continuing, you might want to read the two papers given above, and these blog posts about them:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;q-k-v-and-attention&quot;&gt;Q, K, V and Attention.&lt;/h1&gt;

&lt;p&gt;A Vision Transformer is composed of a few Encoding blocks, where every block has:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A few attention heads, that are responsible, for every patch representation, for fusing information from other patches in the image.&lt;/li&gt;
  &lt;li&gt;An MLP that transforms every patch representation into a higher level feature representation.&lt;/li&gt;
  &lt;li&gt;Both have residual connections.
&lt;strong&gt;And we will see them in action!&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s as simple as this, taken from Ross Wightman’s &lt;a href=&quot;https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py&quot;&gt;Amazing Pytorch Image Models package implementation of vision transformers&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Inside every attention head (the ‘Deit Tiny’ model has 3 attention heads in every layer), the players are Q,k and V.&lt;/p&gt;

&lt;p&gt;The shape of each of these are - &lt;code class=&quot;highlighter-rouge&quot;&gt;3x197x64&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There are 3 attention heads.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each attention heads sees 197 tokens.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every token has a feature representation of length 64.&lt;/p&gt;

    &lt;p&gt;Among these 197 tokens, 196 represent the original image 14x14=196 image patches, and the first token represents a class token that flows through the Transformer, and will be used at the end to make the prediction.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;If for every attention head separately, we look inside the second dimension with 197 tokens, we can peek at the last 14x14=196 tokens.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This gives us an image of size 14x14x64 which we can then visualize.&lt;/p&gt;

&lt;p&gt;The rows of Q and K, are 64 length feature that represents a location in the image.&lt;/p&gt;

&lt;p&gt;We can then think of Q, K and V in the next way:
For every image patch with &lt;script type=&quot;math/tex&quot;&gt;q_i&lt;/script&gt;, Information is going to flow from locations in the image that have keys &lt;script type=&quot;math/tex&quot;&gt;k_j&lt;/script&gt; that are similar to that &lt;script type=&quot;math/tex&quot;&gt;q_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;image from http://jalammar.github.io/illustrated-transformer&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;visual-examples-of-k-and-q---different-patterns-of-information-flowing&quot;&gt;Visual Examples of K and Q - different patterns of information flowing&lt;/h1&gt;

&lt;p&gt;Lets look at an example.&lt;/p&gt;

&lt;p&gt;The input to the network is this image of a plane:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/vit/plane2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can now look at the Q and K images in different layers, and visualize them for one of the 64 channels c.&lt;/p&gt;

&lt;p&gt;This activation vector is going to be a 14x14 image, with positive and negative values, that seem to be in the range [-5, 5].&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{ic}&lt;/script&gt; is the value of the Query feature vector for one of the locations i in the image, in channel c.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;k_{jc}&lt;/script&gt; is the value of the Key feature vector for one of the locations j in the image, in channel c.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Here is a tricky part:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For every location j in K (remember that it comes from one of the 14x14 patches in the original image), we can ask “how is that location going to spread information to other parts of the image?”&lt;/p&gt;

&lt;p&gt;Since we take the dot product between the token vectors (every &lt;script type=&quot;math/tex&quot;&gt;q_{i}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;k_{j}&lt;/script&gt;), there are two scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Two tokens, in the same channel c, &lt;script type=&quot;math/tex&quot;&gt;q_{ic}&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;k_{jc}&lt;/script&gt;, have the same sign (both are positive or negative)- their multiplication is positive.&lt;/p&gt;

    &lt;p&gt;This means that the image location j and channel c - &lt;script type=&quot;math/tex&quot;&gt;k_{jc}&lt;/script&gt; - is going to contribute to flowing information into that image location &lt;script type=&quot;math/tex&quot;&gt;q_{i}&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Two tokens, in the same channel c, &lt;script type=&quot;math/tex&quot;&gt;q_{ic}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;k_{jc}&lt;/script&gt;, have different signs (one is positive and one is negative)- their multiplication is negative.&lt;/p&gt;

    &lt;p&gt;This means that the image j location and channel c  - &lt;script type=&quot;math/tex&quot;&gt;k_{jc}&lt;/script&gt; - is NOT going to contribute to flowing information into that image location &lt;script type=&quot;math/tex&quot;&gt;q_{i}&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To contrast the negative and positive pixels, we’re going to pass every image through a &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.nn.Sigmoid()&lt;/code&gt; layer (the bright values are positive, the dark values are negative).&lt;/p&gt;

&lt;p&gt;From looking at the Q,K visualizations for different channels I think there are kind of two patterns that emerge.&lt;/p&gt;

&lt;h2 id=&quot;pattern-1---the-information-flows-in-one-direction&quot;&gt;Pattern 1 - The information flows in one direction&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Layer 8, channel 26, first attention head:&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Query image&lt;/th&gt;
      &lt;th&gt;Key image&lt;/th&gt;
      &lt;th&gt;Original&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;../assets/vit/q_k/layer_8_channel_26_q.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;../assets/vit/q_k/layer_8_channel_26_k.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;../assets/vit/plane2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;The key image highlights the Airplane.&lt;/li&gt;
  &lt;li&gt;The query image highlights all the image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For most locations in the Query image, since they are positive, information is going to flow to them only from the positive locations in the Key image - that come from the Airplane.&lt;/p&gt;

&lt;p&gt;Q, K here are telling us -&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;We found an airplane, and we want all the locations in the image to know about this!&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;pattern-2---the-information-flows-in-two-directions&quot;&gt;Pattern 2 - The information flows in two directions&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Layer 11, channel 59, first attention head:&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Query image&lt;/th&gt;
      &lt;th&gt;Key image&lt;/th&gt;
      &lt;th&gt;Original&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;../assets/vit/q_k/layer_11_channel_59_q.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;../assets/vit/q_k/layer_11_channel_59_k.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;../assets/vit/plane2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;The Query image highlights mainly the bottom part of the Airplane.&lt;/li&gt;
  &lt;li&gt;The Key image is negative in the top part of the Airplane.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The information flows in two directions here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The top part of the plane (negative values in the Key) is going to spread into all the image (negative values in the Query).&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Hey we found this plane, lets tell the rest of the image about it.&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Information from the “Non Plane” parts of the image (positive values in the Key) is going to flow into the bottom part of the Plane (positive values in the Query).&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Lets tell the plane more about what's around it.&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;how-do-the-attention-activations-look-like-for-the-class-token-throughout-the-network&quot;&gt;How do the Attention Activations look like for the class token throughout the network?&lt;/h1&gt;

&lt;p&gt;Another thing we can do is visualize how the attention flows for the class token, in different layers in the network.&lt;/p&gt;

&lt;p&gt;Since we have multiple attention heads, to keep it simple we will just look at the first one.&lt;/p&gt;

&lt;p&gt;The attention matrix (&lt;script type=&quot;math/tex&quot;&gt;Q*K^T&lt;/script&gt;) has a shape of 197x197.&lt;/p&gt;

&lt;p&gt;If we look at the first row (shape 197), and discard the first value (left with shape 196=14x14) that’s how the information flows from the different locations in the image to the class token.&lt;/p&gt;

&lt;p&gt;Here is how the class attention activations looks like through the layers:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/vit/attention_images/0.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/1.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/2.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/3.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/4.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/5.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/6.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/7.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/8.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/9.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/10.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks like from layer 7 the network was able to segment the plane pretty well.&lt;/p&gt;

&lt;p&gt;However - if we look at consecutive layers, some plane parts are lost, and then re-appear again:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/vit/attention_images/9.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/10.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../assets/vit/attention_images/11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;And we can thank the residual connections for this!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Although the attention suddenly discarded parts of the plane (the middle image above), we don’t loose that information since we have a residual connection from the previous layer.&lt;/p&gt;

&lt;h1 id=&quot;attention-rollout&quot;&gt;Attention Rollout&lt;/h1&gt;

&lt;p&gt;The images above show us how individual activations look like, but they don’t show us how the attention flows from the start to the end throughout the Transformer.&lt;/p&gt;

&lt;p&gt;To quantify this we can use a technique called “Attention Rollout” from &lt;a href=&quot;https://arxiv.org/abs/2005.00928&quot;&gt;Quantifying Attention Flow in Transformers, by Samira Abnar and Willem Zuidema&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is also what the authors at &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt; suggested.&lt;/p&gt;

&lt;p&gt;At every Transformer block we get an attention Matrix &lt;script type=&quot;math/tex&quot;&gt;A_{ij}&lt;/script&gt; that defines how much attention is going to flow from token j in the previous layer to token i in the next layer.&lt;/p&gt;

&lt;p&gt;We can multiply the Matrices between every two layers, to get the total attention flow between them.&lt;/p&gt;

&lt;p&gt;However - we also have the residual connections (like we saw in the previous section).&lt;/p&gt;

&lt;p&gt;We can model them by adding the identity matrix I to the layer Attention matrices:  &lt;script type=&quot;math/tex&quot;&gt;A_{ij} + I&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;we-have-multiple-attention-heads-what-do-we-do-about-them&quot;&gt;We have multiple attention heads. What do we do about them?&lt;/h4&gt;

&lt;p&gt;The Attention rollout paper suggests taking the average of the heads.
As we will see, it can make sense using other choices: like the minimum, the maximum, or using different weights.&lt;/p&gt;

&lt;p&gt;Finally we get a way to recursively compute the Attention Rollout matrix at layer L:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AttentionRollout_{L} = (A_L + I ) \dot AttentionRollout_{L-1}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;We also have to normalize the rows, to keep the total attention flow 1.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;modifications-to-get-attention-rollout-working-with-vision-transformers&quot;&gt;Modifications to get Attention Rollout working with Vision Transformers&lt;/h1&gt;

&lt;p&gt;I implemented this and ran this on the recent ‘Data Efficient’ models from Facebook, but the results weren’t quite as nice as in the &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;,  paper.&lt;/p&gt;

&lt;p&gt;Results were very noisy, and the attention doesn’t seem to focus only on the interesting part of the image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/vit-explain/raw/main/examples/both_attention_rollout_0.000_mean.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Trying to get this to work, I noticed two things:&lt;/p&gt;

&lt;h2 id=&quot;the-way-we-fuse-the-attention-heads-matters&quot;&gt;The way we fuse the attention heads matters&lt;/h2&gt;

&lt;p&gt;For example, here is how the result looks if we take minimum value among the attention heads, instead of the mean value as suggested in the Attention Rollout paper:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Image&lt;/th&gt;
      &lt;th&gt;Mean Fusion&lt;/th&gt;
      &lt;th&gt;Min Fusion&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/both.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/both_attention_rollout_0.000_mean.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/both_attention_rollout_0.000_min.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Different attention heads look at different things, so I guess taking the minimum removes noise by finding their common denominator.&lt;/p&gt;

&lt;p&gt;However, combined with discarding low attention pixels (next section), fusing the attention heads with the maximum operator seems to work best.&lt;/p&gt;

&lt;h2 id=&quot;we-can-focus-only-on-the-top-attentions-and-discard-the-rest&quot;&gt;We can focus only on the top attentions, and discard the rest&lt;/h2&gt;

&lt;p&gt;Discarding the lowest attention values has a huge effect in how the results look like.&lt;/p&gt;

&lt;p&gt;Here is it how it looks as we increase the portion of attention pixels we discard:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/both_discard_ratio.gif&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/plane_discard_ratio.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, the more pixels we remove, we are able to better isolate the salient object in the image.&lt;/p&gt;

&lt;p&gt;Finally, here is how it looks like for a few different images:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Image&lt;/th&gt;
      &lt;th&gt;Vanilla Attention Rollout&lt;/th&gt;
      &lt;th&gt;With discarding lowest pixels + max fusion&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/both.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/both_attention_rollout_0.000_mean.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/both_attention_rollout_0.990_max.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/plane.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/plane_attention_rollout_0.000_mean.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/plane_attention_rollout_0.900_max.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/dogbird.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/dogbird_attention_rollout_0.000_mean.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/dogbird_attention_rollout_0.900_max.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/plane2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/plane2_attention_rollout_0.000_mean.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/plane2_attention_rollout_0.900_max.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;gradient-attention-rollout-for-class-specific-explainability&quot;&gt;Gradient Attention Rollout for Class Specific Explainability&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Edit - it turns out there is another technique out there about this!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Apart from what I implemented below, I refer you to Hila Chefer’s &lt;a href=&quot;https://arxiv.org/pdf/2012.09838v1.pdf&quot;&gt;Transformer Interpretability Beyond Attention Visualization&lt;/a&gt; and their &lt;a href=&quot;https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Fhila-chefer%2FTransformer-Explainability%3ABKmLV7ssfTVusVFB51ifZBGq4ec&amp;amp;cuid=4756419&quot;&gt;github repo.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Another question we can ask is - “What in the image contributes to a higher output score in category 42?”&lt;/p&gt;

&lt;p&gt;Or in other words, Class Specific Explainability.&lt;/p&gt;

&lt;p&gt;When fusing the attention heads in every layer, we could just weight all the attentions (in the current implementation it’s the attentions after the softmax, but maybe it makes sense to change that) by the target class gradient, and then take the average among the attention heads.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_{ij} * grad_{ij}&lt;/script&gt;

&lt;h2 id=&quot;where-does-the-transformer-see-a-dog-category-243-and-a-cat-category-282&quot;&gt;Where does the Transformer see a Dog (category 243), and a Cat (category 282)?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/both_grad_rollout_243_0.900_max.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/both_grad_rollout_282_0.900_max.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;where-does-the-transformer-see-a-musket-dog-category-161-and-a-parrot-category-87&quot;&gt;Where does the Transformer see a Musket dog (category 161) and a Parrot (category 87)?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/dogbird_grad_rollout_161_0.900_max.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/vit-explain/main/examples/dogbird_grad_rollout_87_0.900_max.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;what-activation-maximization-tells-us&quot;&gt;What Activation Maximization Tells us&lt;/h1&gt;

&lt;p&gt;Another thing we can do, is apply Activation Maximization, to find the kind of image inputs that maximize different parts in the network.&lt;/p&gt;

&lt;p&gt;In Vision Transformers the images are split into 14x14 independent patches (that represent 16x16 pixels).&lt;/p&gt;

&lt;p&gt;We also see this in the Activation Maximization result below- instead of getting a continuous image, we get 14x14 patches.&lt;/p&gt;

&lt;p&gt;Since the positional embeddings are added to the inputs, nearby patches should get more similar outputs.&lt;/p&gt;

&lt;p&gt;I think you can see this in the image below - many neighboring patches look similar, but they also have a discontinuity between them.&lt;/p&gt;

&lt;p&gt;I guess future work can be about using some kind of a spatial continuity constraint between the patches here (and maybe also incorporate that into how the transformers process the images).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/vit/activation_maximization/k_100.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this post we applied Explainability techniques for Vision Transformers.&lt;/p&gt;

&lt;p&gt;This was my attempt to try to better understand how are they working and what’s going on inside them.&lt;/p&gt;

&lt;p&gt;You can access the code here: https://github.com/jacobgil/vit-explain&lt;/p&gt;

&lt;p&gt;I hope you enjoyed.&lt;/p&gt;</content><category term="Explainability" /><category term="Deep Learning" /></entry><entry><title type="html">Overview of Active Learning for Deep Learning</title><link href="http://localhost:4000/deeplearning/activelearning" rel="alternate" type="text/html" title="Overview of Active Learning for Deep Learning" /><published>2020-02-21T22:10:33+02:00</published><updated>2020-02-21T22:10:33+02:00</updated><id>http://localhost:4000/deeplearning/overview-of-activelearning</id><content type="html" xml:base="http://localhost:4000/deeplearning/activelearning">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;table-of-contents&quot;&gt;Table Of Contents&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#motivation&quot;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#active-learning--higher-accuracy-with-less-data-annotation&quot;&gt;Active Learning: Higher accuracy with less data annotation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#the-unlabeled-pool-scenario&quot;&gt;The unlabeled pool scenario&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#how-should-we-rank-the-images-for-annotation--&quot;&gt;How should we rank the images for annotation ?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#acquisition-functions-for-uncertainty-sampling---examples&quot;&gt;Acquisition functions for uncertainty sampling - examples&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#query-by-committee-qbc&quot;&gt;Query by committee QBC&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#active-learning-for-deep-learning&quot;&gt;Active Learning for Deep Learning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#deep-bayesian-active-learning-with-image-data&quot;&gt;Deep Bayesian Active Learning with Image Data&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#monte-carlo-dropout&quot;&gt;Monte Carlo dropout&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#plugging-mc-dropout-into-the-entropy-acquisition-function&quot;&gt;Plugging MC Dropout into the Entropy acquisition function&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#bayesian-active-learning-by-disagreement-bald&quot;&gt;Bayesian Active Learning by Disagreement BALD&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#learning-loss-for-active-learning&quot;&gt;Learning Loss for Active Learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mode-collapse-in-active-learning&quot;&gt;Mode collapse in active learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#batch-aware-methods&quot;&gt;Batch aware methods&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#active-learning-for-convolutional-neural-networks--a-core-set-approach&quot;&gt;Active Learning for Convolutional Neural Networks: A Core-Set Approach&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#batchbald&quot;&gt;BatchBALD&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#diverse-mini-batch-active-learning&quot;&gt;Diverse mini-batch Active Learning&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#deep-batch-active-learning-by-diverse--uncertain-gradient-lower-bounds&quot;&gt;Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;To train supervised machine learning algorithms, we need:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Data and annotations for the data.&lt;/li&gt;
  &lt;li&gt;The ability to “learn” from the data, usually by optimizing a model so it fits the data and its annotations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Most of the focus of the machine learning community is about (2), creating better algorithms for learning from data.
But getting useful annotated datasets is difficult. Really difficult. It can be expensive, time consuming, and you still end up with problems like annotations missing from some categories.&lt;/p&gt;

&lt;p&gt;I think that being able to build practical machine learning systems is a lot about tools to annotate data, and that a lot of the future innovation in building systems that solve real problems will be about being able to annotate high quality datasets quickly.&lt;/p&gt;

&lt;p&gt;Active Learning is a great building block for this, and is under utilized in my opinion.&lt;/p&gt;

&lt;p&gt;In this post I will give a short introduction to Classical Active Learning, and then go over several papers that focus on Active Learning for Deep Learning.&lt;/p&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;

&lt;p&gt;In many scenarios we will actually have access to a lot of data, but it will be infeasible to annotate everything.
Just a few examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A billion images scraped from the internet.&lt;/li&gt;
  &lt;li&gt;A video recorded from a dash cam that is a month long.&lt;/li&gt;
  &lt;li&gt;A scanned biopsy that has a million cells inside it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are two closely related fields that help us deal with these scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Semi-supervised Learning&lt;/em&gt;.
Exploit the unannotated data to get better feature representations and improve the algorithms learned on the annotated data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Active Learning&lt;/em&gt;.
 Choose the data that is going to be annotated.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;active-learning-higher-accuracy-with-less-data-annotation&quot;&gt;Active Learning: Higher accuracy with less data annotation&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;../assets/graph.png&quot; alt=&quot;graph&quot; height=&quot;300&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image from https://arxiv.org/abs/1703.02910&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The image above is a typical image in a paper about active learning. 
The x axis is the size of the dataset. The y axis is accuracy on the test set.&lt;/p&gt;

&lt;p&gt;A good active learning algorithm is able to cleverly select that data we are going to annotate, so that when we train on it, our model will be better than if we trained on other data.&lt;/p&gt;

&lt;p&gt;So if for example we have the constraint of being able to annotate only 400 images, the goal of active learning will be to select the best 400 images to annotate.&lt;/p&gt;

&lt;h2 id=&quot;the-unlabeled-pool-scenario&quot;&gt;The unlabeled pool scenario&lt;/h2&gt;

&lt;p&gt;The most common scenario considered in active learning literature, which is also most similar to what happens in real life problems, is the unlabeled pool scenario.
This is a good place to say that I’m going to interchange the words label and annotation, and I’m going to assume we’re using images, because the principles will be the same for everything else.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The unlabeled pool scenario&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There is a large pool of unlabeled data.&lt;/li&gt;
  &lt;li&gt;We play a round based game.&lt;/li&gt;
  &lt;li&gt;Every round we run an algorithm that chooses the best image to annotate from the pool.
This is done by &lt;strong&gt;ranking&lt;/strong&gt; the images left in the unlabeled dataset, and choosing the highest ranking image.&lt;/li&gt;
  &lt;li&gt;We annotate the selected image and add it to the training set.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Then we train a  model.&lt;/p&gt;

    &lt;p&gt;In active learning papers at this stage usually the model is trained from scratch on the new dataset, but in real-life you’re probably going to continue from the previous model you had.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Repeat until the annotation budget is over.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-should-we-rank-the-images-for-annotation-&quot;&gt;How should we rank the images for annotation ?&lt;/h2&gt;

&lt;p&gt;At this point I want to write something obvious, that’s probably worth mentioning anyway. A really good strategy would be to select the images the model is just wrong about.
But we can’t do that since we don’t know the real labels.&lt;/p&gt;

&lt;p&gt;There are two main approaches that most of the active learning works follow.
Sometimes they are a combination of the two.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Uncertainty sampling&lt;/strong&gt;: Try to find images the model isn’t certain about, as a proxy of the model being wrong about the image.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Diversity sampling&lt;/strong&gt;: Try to find images that represent the diversity existing in the images that weren’t annotated yet.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A function that gets an image and returns a ranking score, is often called an “acquisition function” in the literature.&lt;/p&gt;

&lt;p&gt;Let’s look a few examples.&lt;/p&gt;

&lt;h2 id=&quot;acquisition-functions-for-uncertainty-sampling---examples&quot;&gt;Acquisition functions for uncertainty sampling - examples&lt;/h2&gt;

&lt;p&gt;Lets look at a few &lt;em&gt;classic&lt;/em&gt; uncertainty acquisition functions. We will cover some more when we get to the deep learning methods.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Entropy &lt;script type=&quot;math/tex&quot;&gt;H(p) = -\sum p_i Log_2(p_i)&lt;/script&gt;&lt;/p&gt;

    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;H( [0.5, 0.5] )&lt;/script&gt; = 1.0&lt;/p&gt;

    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;H( [1.0, 0.0] )&lt;/script&gt; = 0.0&lt;/p&gt;

    &lt;p&gt;This is probably the most important example to understand.
The idea is that when the model output is the same for all the categories, it is completely confused between the categories.&lt;/p&gt;

    &lt;p&gt;The rank will be highest in this case, because the entropy function is maximized the all it’s inputs are equal, so we will select the image.
&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; will grow as the probabilities p tend to be more uniform, and will shrink when fewer of the categories tend to get higher values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Variation Ratio: &lt;script type=&quot;math/tex&quot;&gt;1-max(p)&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The difference between the largest output from the model, and the second largest output.&lt;/li&gt;
  &lt;li&gt;In SVMs: the distance of a point from the hyperplane.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;query-by-committee-qbc&quot;&gt;Query by committee QBC&lt;/h2&gt;

&lt;p&gt;Another concept from classical active learning papers, is QBC. 
The idea here is that instead of measuring the uncertainty of a single model, we can train an ensemble of many different models (maybe with different seeds, or hyper-parameters, or structures).
Then for a given image, we can check if the output changes a lot between models. If it does, it means the models aren’t very consistent about this image, and some of them aren’t doing a good job on this image.&lt;/p&gt;

&lt;p&gt;In the typical QBC implementation, every model decides the output category and votes for it, and a vector with the vote count is created.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Some examples of how his vote vector can be used as an uncertainty measure.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;(Minimize) The difference between the two categories with the most votes.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;(Maximize) Vote Entropy: &lt;script type=&quot;math/tex&quot;&gt;-\frac{V(c)}{C}Log(\frac{V(c)}{C})&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;active-learning-for-deep-learning&quot;&gt;Active Learning for Deep Learning&lt;/h1&gt;

&lt;p&gt;Combining Active Learning and deep learning is hard.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Deep neural networks aren’t really good at telling when they are not sure.
The output from the final softmax layer tends to be over confident.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Deep neural networks are computationally heavy, so you usually want to select a &lt;strong&gt;batch&lt;/strong&gt; with many images to annotate at once.&lt;/p&gt;

    &lt;p&gt;But the acquisition functions we saw so far tell us how to select the single best image, not a batch of images.&lt;/p&gt;

    &lt;p&gt;How should we select a batch of images at once? That’s an open research question, and we will cover a few &lt;em&gt;batch aware&lt;/em&gt; methods below.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now lets cover a few papers about Active Learning for Deep Learning.&lt;/p&gt;

&lt;h2 id=&quot;deep-bayesian-active-learning-with-image-data&quot;&gt;Deep Bayesian Active Learning with Image Data&lt;/h2&gt;

&lt;p&gt;The paper: &lt;a href=&quot;https://arxiv.org/abs/1703.02910&quot;&gt;https://arxiv.org/abs/1703.02910&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In my opinion this is the currently the most important paper about active learning for deep learning, so we are going to cover this in detail.&lt;/p&gt;

&lt;p&gt;The idea is that Bayesian neural networks give better uncertainty measures.&lt;/p&gt;

&lt;p&gt;In a Bayesian neural network, every parameter in the model is sampled from a distribution. Then, when doing inference, we need to integrate over all the possible parameters. So we’re using an ensemble of infinite different networks to compute the output.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;An uncertainty measure from a single network might be flawed (maybe it’s over confident in the output), but the idea is that going over many networks is going to improve that.&lt;/p&gt;

&lt;p&gt;Intuitively, if most models agree about one of the categories, the ensembled network will have high confidence for that category. If they disagree, we will get large outputs for several of the categories.&lt;/p&gt;

&lt;p&gt;It’s intractable to integrate over all possible parameter values in the distribution, so instead Monte Carlo integration can be used.&lt;/p&gt;

&lt;h3 id=&quot;monte-carlo-dropout&quot;&gt;Monte Carlo dropout&lt;/h3&gt;

&lt;p&gt;With Monte Carlo dropout, the idea is that we will simulate a case where every neuron output has a Bernoulli prior, multiplied by some value M (the actual value of that neuron output).&lt;/p&gt;

&lt;p&gt;So a parameter &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is going to be 0 with some probability p, and &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; otherwise.&lt;/p&gt;

&lt;p&gt;Now we can sample from the neuron priors by running the network and applying dropout at test time.
If we apply dropout many times and sum the results, we’re doing Monte Carlo integration.
Lets break this into steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y=c|x) = \int p(y=c|x,\omega)p(w)dw&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We have a Bayesian neural network and an input image x. To get the output for the category c, we’re going over all the possible weight configurations, weighting every configuration by its probability.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\approx \int p(y=c|x,\omega)*q^*(w)dw&lt;/script&gt;

    &lt;p&gt;We don’t know the actual real parameter distribution, but we can approximate them assuming they belong to the Bernoulli distribution. We can simulate the Bernoulli distribution, simply by using Dropout.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\approx \frac{1}{T}\sum_t p(y|x,\omega_t) = \frac{1}{T}\sum_t p^t_c&lt;/script&gt;

    &lt;p&gt;To further approximate, we apply Monte Carlo integration, by running the network with dropout many times, and summing the results.
 As T gets larger, the approximation will get better.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;This gives us a simple recipe: to approximate the output probability for every category, run the model many times with dropout and taking the average of all the runs.&lt;/p&gt;

&lt;h3 id=&quot;plugging-mc-dropout-into-the-entropy-acquisition-function&quot;&gt;Plugging MC Dropout into the Entropy acquisition function&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;An example of Uncertainty Sampling&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The Entropy uncertainty acquisition function is:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_c p(y=c|x)Log(p(y=c|x))&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If we plug the approximation from above, we get:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;H \approx-\sum_c(\frac{1}{T}\sum_tp_c^t)Log(\frac{1}{T}\sum_tp_c^t)&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We need to run the network multiple time with dropout, average the outputs, and take the entropy.&lt;/p&gt;

&lt;p&gt;Lets see how this measure behaves in two important cases that achieve a high &lt;strong&gt;H&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Every time we run the model with dropout, it’s confident about a different category.
This means that some of the models were very wrong about the input X.
These models are each associated with a set of parameters that survived the dropout.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;If we select this image, we can correct those parameters that caused the models to be wrong.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Many of the runs end up being not so confident, and are confused between different categories.&lt;/p&gt;

    &lt;p&gt;But the many models we sampled actually agree about the input X. They all think it’s a confusing example. 
For example, who knows, maybe there is a large occlusion in the image and we can’t clearly see the object we’re looking for. How can we ever be correct on this image? If we label this image, how should the model change?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This leads us to a modification that handles the second case:&lt;/p&gt;

&lt;h3 id=&quot;bayesian-active-learning-by-disagreement-bald&quot;&gt;Bayesian Active Learning by Disagreement BALD&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;An example of Uncertainty Sampling&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Uncertainty sampling, the ultimate goal would be to find images the model is wrong about.&lt;/p&gt;

&lt;p&gt;We can’t do that since we don’t know the labels. So the idea in BALD is to instead find examples for which many of the different sampled networks are wrong about.&lt;/p&gt;

&lt;p&gt;If we sample many networks using MC Dropout, and they disagree about the output, this means that some of them are wrong.&lt;/p&gt;

&lt;p&gt;Lets get to the math.  The objective is to find the image that maximizes the mutual information between the model output, and the model parameters.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(y; \omega | x, D_{train}) = H(y | x, D_{train}) - E_{p(\omega|D_{train})} [H(y|x, \omega, D_{train})]&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The first term looks for images that have high entropy in the average output.&lt;/li&gt;
  &lt;li&gt;The second term penalizes images where many of the sampled models are not confident about.
This keeps only images where the models &lt;strong&gt;disagree&lt;/strong&gt; on.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we plug in the Monte Carlo approximation again, we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(y; \omega | x, D_{train}) \approx-\sum_c(\frac{1}{T}\sum_tp_c^t)Log(\frac{1}{T}\sum_tp_c^t) + \frac 1 T \sum_{t, c} p_c^t Log p_c^t&lt;/script&gt;

&lt;p&gt;To get the first term, we make many runs, average the output, and measure the entropy.&lt;/p&gt;

&lt;p&gt;To get the second term, we make many runs, measure the entropy of every run, and take the average.&lt;/p&gt;

&lt;h2 id=&quot;learning-loss-for-active-learning&quot;&gt;Learning Loss for Active Learning&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;An example of Uncertainty Sampling&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The paper: https://arxiv.org/abs/1905.03677&lt;/p&gt;

&lt;p&gt;The main idea here is to try to predict what would be the loss of the learning algorithm for a given input.&lt;/p&gt;

&lt;p&gt;A higher loss means its a more difficult image and should be annotated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/loss.png&quot; alt=&quot;predicting the loss&quot; /&gt;&lt;/p&gt;

&lt;p&gt;They extract features from intermediate layers and combine them to predict the network loss.&lt;/p&gt;

&lt;p&gt;To learn the loss, you could just add an additional term to the loss: &lt;strong&gt;MSE(Real loss, predicted loss)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;But during training, the scale of the loss is changing all the time (since the loss is usually going down), and its difficult to learn.&lt;/p&gt;

&lt;p&gt;Instead, in this paper they suggest comparing the predicted losses between images.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Every batch of size B is split into B/2 pairs of images.&lt;/li&gt;
  &lt;li&gt;In every pair, the loss is predicted for both images.&lt;/li&gt;
  &lt;li&gt;The two losses are compared with a hinge loss, requiring that the two losses should have a distance of at least &lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt;  between them.
&lt;img src=&quot;../assets/loss2.png&quot; alt=&quot;rank&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then during the active learning rounds, you select the images with the highest loss.&lt;/p&gt;

&lt;h2 id=&quot;mode-collapse-in-active-learning&quot;&gt;Mode collapse in active learning&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/holes.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image from https://arxiv.org/abs/1811.03897&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A possible problem with all these methods, is that during training your dataset won’t necessarily be balanced. The acquisition strategy might favor bringing more images from some of the categories.&lt;/p&gt;

&lt;p&gt;https://arxiv.org/abs/1811.03897 showed this happens for common uncertainty based acquisition functions: random acquisition ends up bringing a much more balanced dataset than active learning.&lt;/p&gt;

&lt;p&gt;I think this is a really nice visualization that shows that scoring images by how difficult they are, won’t be 
good enough, and that we have to somehow also optimize for having diverse a diverse dataset.&lt;/p&gt;

&lt;p&gt;Of course it’s not enough to have enough images from all the categories. Ideally we would capture the sub-categories inside every category as well.&lt;/p&gt;

&lt;p&gt;This leads us to the next part about “Batch aware” methods, where we will see some works that try to combine Uncertainty sampling and Diversity sampling.&lt;/p&gt;

&lt;h2 id=&quot;batch-aware-methods&quot;&gt;Batch aware methods&lt;/h2&gt;

&lt;p&gt;Most of the active learning works selects a single image, at every active learning round.&lt;/p&gt;

&lt;p&gt;However for expensive/data hungry methods like Deep Learning we need to select a batch with many images at every round. After all, we’re going to train on the entire new dataset and not just only on the new images, so we can’t afford to train again every time a single new image is added to the dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Datasets can have many near duplicates.
If we just select the top ranking images, we might select many near duplicate images that all rank high.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/bald.png&quot; alt=&quot;batchbald&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image from https://arxiv.org/abs/1906.08158&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Next we are going to go over a few “batch aware” methods, that try to select a good batch of images.&lt;/p&gt;

&lt;h3 id=&quot;active-learning-for-convolutional-neural-networks-a-core-set-approach&quot;&gt;Active Learning for Convolutional Neural Networks: A Core-Set Approach&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;An example of Diversity sampling&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The paper: https://arxiv.org/abs/1708.00489&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/coreset.png&quot; alt=&quot;Core Sets&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This paper is an example from the diversity sampling family of active learning algorithms, and is also a “batch aware” method, that tries to choose a batch of B images at once.&lt;/p&gt;

&lt;p&gt;The main idea here is that we want the training set to capture the diversity in the dataset. To find the data that isn’t represented well yet by the training set, we need to find the “Core Set” at every step.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The core set&lt;/strong&gt;: 
&lt;em&gt;B images such that when added to the training set, the distance between an image in the unlabeled pool and it’s closest image in the training set, will be minimized.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Finding the ideal B images is NP hard. Instead a simple greedy approximation is used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find an image X with the largest distance from the training set D, add x to D. Repeat B times&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How do we define distances between images? It’s an open question with place for creativity.&lt;/p&gt;

&lt;p&gt;For images they use the Euclidian distance between feature vectors extracted from the end of the network.&lt;/p&gt;

&lt;h3 id=&quot;batchbald&quot;&gt;BatchBALD&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;An example of combining Uncertainty and Diversity Sampling&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The paper: https://arxiv.org/abs/1906.08158&lt;/li&gt;
  &lt;li&gt;The authors blog post on this: https://oatml.cs.ox.ac.uk/blog/2019/06/24/batchbald.html&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In BALD, the acquisition function for a single data point was the mutual information between the model parameters and the model output:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(y; \omega | x, D_{train}) = H(y | x, D_{train}) - E_{p(\omega|D_{train})} [H(y|x, \omega, D_{train})]&lt;/script&gt;

&lt;p&gt;In this work the goal is to select a batch of images at once, so they the change the acquisition function to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(y_1,...,y_B; \omega | x_1,..,x_B, D_{train}) = H(y_1,...,y_B | x_1,...,x_B, D_{train}) - E_{p(\omega|D_{train})} [H(y_1,...,y_B|x_1,...,x_B, \omega, D_{train})]&lt;/script&gt;

&lt;p&gt;Lets say we have a batch size of 3, and have two images that have a high BALD acquisition score: a and b, and another image that is a duplicate of a: a’.&lt;/p&gt;

&lt;p&gt;If we select the batch images one by one, we will select a, a’ and b, since they all have high scores.&lt;/p&gt;

&lt;p&gt;a’ is redundant since it already exists in the batch.&lt;/p&gt;

&lt;p&gt;BatchBald won’t select a’, since it doesn’t contribute anything to the total mutual information:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I(a,b,a') = I(a, b)&lt;/script&gt;

&lt;p&gt;This encourages adding informative images that are different from the rest of the images in the batch.&lt;/p&gt;

&lt;p&gt;Approximating 
&lt;script type=&quot;math/tex&quot;&gt;H(y_1,...,y_B | x_1,...,x_B, D_{train})&lt;/script&gt; 
involves quite a bit of math and details, so refer to the paper for the details.&lt;/p&gt;

&lt;h3 id=&quot;diverse-mini-batch-active-learning&quot;&gt;Diverse mini-batch Active Learning&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;An example of combining Uncertainty and Diversity Sampling&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Paper: https://arxiv.org/abs/1901.05954&lt;/p&gt;

&lt;p&gt;The idea here is to combine uncertainty sampling and diversity sampling.&lt;/p&gt;

&lt;p&gt;For diversity sampling, they cluster the data into K clusters using K-means (in the case of images, as features they use features extracted from intermediate layers from the neural network classifier).&lt;/p&gt;

&lt;p&gt;Then they select images that are closest to the centers of each of the clusters, to make sure the sampled images are diverse.&lt;/p&gt;

&lt;p&gt;To incorporate uncertainty sampling and select difficult images, they use &lt;strong&gt;weighted K-means&lt;/strong&gt;, where every image is assigned a weight from an uncertainty acquisition function.&lt;/p&gt;

&lt;p&gt;Since K-means can be slow, they pre-filter the unlabeled images to keep the top  &lt;script type=&quot;math/tex&quot;&gt;\beta*K&lt;/script&gt;  images with the highest uncertainty scores, and do the clustering only on them (&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; is typically 10).&lt;/p&gt;

&lt;h3 id=&quot;deep-batch-active-learning-by-diverse-uncertain-gradient-lower-bounds&quot;&gt;Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;An example of combining Uncertainty and Diversity Sampling&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is really similar in nature to the previous paper, and in my opinion also to Core Sets, but has a unique twist.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For uncertainty sampling, instead of using the model output like is usually done, they compute the gradient of the predicted category, with respect to the parameters of the last layer. There are many parameters, so the gradient is a vector.&lt;/p&gt;

    &lt;p&gt;They call these.. &lt;em&gt;drums roll..&lt;/em&gt;  &lt;strong&gt;&lt;em&gt;gradient embeddings&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

    &lt;p&gt;The rational here is that when the gradient norm is large, the parameters need to change a lot to be more confident about the category.&lt;/p&gt;

    &lt;p&gt;They give the rational that it’s more natural to use gradients as uncertainty measures in neural networks, since gradients are used to train the network.&lt;/p&gt;

    &lt;p&gt;But this also gives us an embedding we can &lt;strong&gt;cluster&lt;/strong&gt; to chose diverse points.
So it kills two birds with one stone: a way to choose uncertain images, and a way to chose diverse images.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;They then proceed to cluster the embeddings to chose diverse points. Instead of using K-means to choose the batch points, they use a K-means initialization algorithm called K-means++, which is much faster to compute.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What’s K-means++? From &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means%2B%2B&quot;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. Choose one center uniformly at random from among the data points.
2. For each data point x, compute D(x), the distance between x and the nearest center that 
has already been chosen.
3. Choose one new data point at random as a new center, using a weighted 
probability distribution where a point x is chosen with probability proportional to D(x)^2.
4. Repeat Steps 2 and 3 until k centers have been chosen.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you recall the Core-Set work, it’s really similar to K-means++ !
In Core-Sets, the embeddings were features from one of the last layers. In this work it’s almost the same thing, but with gradient embeddings.&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Thanks for reading. We went over active learning methods for Deep Learning.&lt;/p&gt;

&lt;p&gt;These methods are really creative, and it was a joy to write.&lt;/p&gt;

&lt;p&gt;We were focusing on images, but these methods can be used for other domains like text.&lt;/p&gt;

&lt;p&gt;I hope this will do some help to demystify active learning for Deep Learning.&lt;/p&gt;</content><category term="Active Learning" /><category term="Deep Learning" /><summary type="html">Overview of different Active Learning algorithms for Deep Learning.</summary></entry><entry><title type="html">Hallucinating faces with Deep Learning</title><link href="http://localhost:4000/deeplearning/hallucinating_faces_dlib_pytorch" rel="alternate" type="text/html" title="Hallucinating faces with Deep Learning" /><published>2018-03-31T15:10:33+03:00</published><updated>2018-03-31T15:10:33+03:00</updated><id>http://localhost:4000/deeplearning/hallucinating-faces-dlib-pytorch</id><content type="html" xml:base="http://localhost:4000/deeplearning/hallucinating_faces_dlib_pytorch">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_4.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_3.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_6.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_5.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_4.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_2.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_3.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_0.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_8.jpg&quot; alt=&quot;&quot; /&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jacobgil/dlib_facedetector_pytorch&quot;&gt;Here is the github repository&lt;/a&gt; with all the code for this post.&lt;/p&gt;

&lt;p&gt;Scroll  to the end if you just want to see images.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In this post I will describe two experiments I did with Dlib’s deep learning face detector:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Porting the model weights to PyTorch, and testing it by detecting faces in a web cam feed.&lt;/li&gt;
  &lt;li&gt;Hallucinating faces using Activation Maximization on the model filters.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Dlib’s &lt;a href=&quot;http://blog.dlib.net/2016/10/easily-create-high-quality-object.html&quot;&gt;deep learning face detector&lt;/a&gt; is one of the most popular open source face detectors. It is used in many open source projects like the &lt;a href=&quot;https://cmusatyalab.github.io/openface/&quot;&gt;open face project&lt;/a&gt;, but also in countless industry applications as well.
It is trained with the clever max margin object detection agorithm that penalizes objects that are not exactly in the center of the scanning window, thus learning non maximum supression, giving very accurate localization.&lt;/p&gt;

&lt;p&gt;This is a good place to say that DLib is a remarkable piece of software, and it’s creator &lt;a href=&quot;https://twitter.com/nulhom&quot;&gt;Davis King&lt;/a&gt; is one of the heros of the internet.&lt;/p&gt;

&lt;p&gt;At this point Dlib only has support for &lt;a href=&quot;https://github.com/davisking/dlib/tree/master/tools/convert_dlib_nets_to_caffe&quot;&gt;converting the model weights to caffe&lt;/a&gt;, so I decided to jump in and add support for converting the face detector model to PyTorch. From PyTorch it can be easily be ported to many other platforms with the &lt;a href=&quot;http://pytorch.org/docs/master/onnx.html&quot;&gt;ONNX format&lt;/a&gt;, so getting dlib’s face detector to work in mobile deep learning frameworks should be straight forward from here.&lt;/p&gt;

&lt;h1 id=&quot;converting-the-model-to-pytorch&quot;&gt;Converting the model to PyTorch&lt;/h1&gt;
&lt;p&gt;The first part here was saving the face detector model in an XML format, using &lt;code class=&quot;highlighter-rouge&quot;&gt;net_to_xml&lt;/code&gt;, like in this &lt;a href=&quot;https://github.com/davisking/dlib/blob/master/examples/dnn_introduction_ex.cpp#L164&quot;&gt;dlib example&lt;/a&gt;.
The XML is fairly easy to parse in python, with each layer’s parameters (like the layer type, padding, kernel size etc) stored in XML attributes, followed by a list of floats for each layer’s biases and weights.
Batch normalization is implemented a bit differently in DLib, without a running mean and running variance as part of the layer parameters, so a running mean and variance of 0 and 1 is used in PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jacobgil/dlib_facedetector_pytorch/blob/master/dlib_torch_converter.py#L6&quot;&gt;get_model&lt;/a&gt; gets the XML path, and returns a PyTorch Sequential model.&lt;/p&gt;

&lt;h1 id=&quot;verifying-it-by-detecting-faces-in-a-webcam&quot;&gt;Verifying it by detecting faces in a webcam&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The purpose of this section was to make sure the ported model is usable. 
 You can skip to the next section for the face hallucinations.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;On a i7 processor, the inference took between 30ms to 150ms on a 640x480 feed from a webcam, depending on the scales used, which isn’t bad at all.
Running it on higher end mobile devices (after porting to ONNX) should give a much faster inference time.&lt;/p&gt;

&lt;p&gt;Dlib’s face detector is a fully convolutional network, that slides over an input image and outputs a score for each window in the image. 
The network is aimed at detecting faces that have a certain size, determined by the receptive field of the network.&lt;/p&gt;

&lt;p&gt;To get scale invariance, Dlib resizes the input images to different sizes, and packs them in a single image with paddings between the scaled images.
This trades off more inference time with scale invariance. 
Inference on the packed larger image gives larger GPU utilization.
Since I was doing this on a CPU, I didn’t really have a motivation for doing the image packing, so instead I just did multiple forward passes on resized images.&lt;/p&gt;

&lt;p&gt;After detection, non maxima suppression is done between the different scales, and the box size is receptive field is multiplied by the scale that best detected the object.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jacobgil/dlib_facedetector_pytorch/blob/master/webcam_example.py&quot;&gt;Here is the code for face detection on a webcam.&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;hallucinating-faces&quot;&gt;Hallucinating faces&lt;/h1&gt;

&lt;p&gt;Now that we have the PyTorch model, we can use activation maximization to find images that cause a large response in specific filters. 
The idea is to perform gradient ascent iterations on the input image pixels, until a large activation in filter output is caused.&lt;/p&gt;

&lt;p&gt;I tried a lot of things until I managed to get this to work. 
Here is a short summary of some of the things I used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The loss function is the center pixel in the filter output.&lt;/strong&gt;
Usually when using activation maximization, the mean output of the filter is used as the loss function. 
To get face images, instead I used the center pixel in the output of the filter.
This is probably related to how the model was trained - if a face is centered in the middle of the window, its score will be highest.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Regularization: Without regularization, the images look extremely noisy&lt;/strong&gt;.  The regularization that worked best here was rotating the image by a random angle (I used a range of [-30, 30]), calculating the image gradients, and then rotating back. A random horizontal flip also seemed to help.
Initially I tried using bilateral filtering on the gradients, and decaying the image by 0.95, but those didn’t really help and the results weren’t as nice.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Peeking at the second last convolutional layer.&lt;/strong&gt;
The output from the last convolutional layer used a combination of the outputs in the one before, and tends to return multiple faces (often in different poses) in the same image. This kind of makes sense, since there are many different types of faces that can all cause a face to be detected.&lt;/p&gt;

    &lt;p&gt;On the other hand, maximizing the second last convolutional layer responses returns single faces, probably because they learned to be much more selective in the kind of faces they respond to.
  Different runs on the same filter often returns different poses and expressions, of the same face.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Solving instead for a low activation, often still returns faces, but they aren’t in the center of the image.&lt;/strong&gt;
This kind of makes sense because the face detector is trained to give a low response if the face has an offset to the window center, treating it as a false positive. So many of the “negative” responses have meanings - They aren’t just random background patterns, they are faces, and face parts, in different areas of the image.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;images-that-maximize-the-activation&quot;&gt;Images that maximize the activation&lt;/h1&gt;
&lt;p&gt;These are selected filters. Some filters did not correspond to faces, or had multiple faces.
For each filter, there were 900 iterations of gradient ascent, repeated 10 times to create 10 different images.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/0_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/1_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/5_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/8_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/9_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/10_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/11_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/12_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/13_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/14_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/17_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/18_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/20_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/23_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/24_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/25_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/positive_images/28_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;images-that-minimize-the-activation&quot;&gt;Images that minimize the activation&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/0_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/1_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/2_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/3_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/4_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/5_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/6_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/7_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/8_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/9_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/10_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/11_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/12_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/13_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/14_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/15_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/16_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/17_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/18_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/19_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/20_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/21_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/22_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/23_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/24_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/25_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/26_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/27_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/28_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_0.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_1.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_2.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_3.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_4.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_5.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_6.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_7.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_8.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/dlib_facedetector_pytorch/master/negative_images/29_9.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><summary type="html">Here is the github repository with all the code for this post.

Scroll  to the end if you just want to see images.



In this post I will describe two experiments I did with Dlib’s deep learning face detector:


  Porting the model weights to PyTorch, and testing it by detecting faces in a web cam feed.
  Hallucinating faces using Activation Maximization on the model filters.


Dlib’s deep learning face detector is one of the most popular open source face detectors. It is used in many open source projects like the open face project, but also in countless industry applications as well.
It is trained with the clever max margin object detection agorithm that penalizes objects that are not exactly in the center of the scanning window, thus learning non maximum supression, giving very accurate localization.

This is a good place to say that DLib is a remarkable piece of software, and it’s creator Davis King is one of the heros of the internet.

At this point Dlib only has support for converting the model weights to caffe, so I decided to jump in and add support for converting the face detector model to PyTorch. From PyTorch it can be easily be ported to many other platforms with the ONNX format, so getting dlib’s face detector to work in mobile deep learning frameworks should be straight forward from here.

Converting the model to PyTorch
The first part here was saving the face detector model in an XML format, using net_to_xml, like in this dlib example.
The XML is fairly easy to parse in python, with each layer’s parameters (like the layer type, padding, kernel size etc) stored in XML attributes, followed by a list of floats for each layer’s biases and weights.
Batch normalization is implemented a bit differently in DLib, without a running mean and running variance as part of the layer parameters, so a running mean and variance of 0 and 1 is used in PyTorch.

get_model gets the XML path, and returns a PyTorch Sequential model.

Verifying it by detecting faces in a webcam

The purpose of this section was to make sure the ported model is usable. 
 You can skip to the next section for the face hallucinations.

On a i7 processor, the inference took between 30ms to 150ms on a 640x480 feed from a webcam, depending on the scales used, which isn’t bad at all.
Running it on higher end mobile devices (after porting to ONNX) should give a much faster inference time.

Dlib’s face detector is a fully convolutional network, that slides over an input image and outputs a score for each window in the image. 
The network is aimed at detecting faces that have a certain size, determined by the receptive field of the network.

To get scale invariance, Dlib resizes the input images to different sizes, and packs them in a single image with paddings between the scaled images.
This trades off more inference time with scale invariance. 
Inference on the packed larger image gives larger GPU utilization.
Since I was doing this on a CPU, I didn’t really have a motivation for doing the image packing, so instead I just did multiple forward passes on resized images.

After detection, non maxima suppression is done between the different scales, and the box size is receptive field is multiplied by the scale that best detected the object.

Here is the code for face detection on a webcam.

Hallucinating faces

Now that we have the PyTorch model, we can use activation maximization to find images that cause a large response in specific filters. 
The idea is to perform gradient ascent iterations on the input image pixels, until a large activation in filter output is caused.

I tried a lot of things until I managed to get this to work. 
Here is a short summary of some of the things I used:


  The loss function is the center pixel in the filter output.
Usually when using activation maximization, the mean output of the filter is used as the loss function. 
To get face images, instead I used the center pixel in the output of the filter.
This is probably related to how the model was trained - if a face is centered in the middle of the window, its score will be highest.
  Regularization: Without regularization, the images look extremely noisy.  The regularization that worked best here was rotating the image by a random angle (I used a range of [-30, 30]), calculating the image gradients, and then rotating back. A random horizontal flip also seemed to help.
Initially I tried using bilateral filtering on the gradients, and decaying the image by 0.95, but those didn’t really help and the results weren’t as nice.
  
    Peeking at the second last convolutional layer.
The output from the last convolutional layer used a combination of the outputs in the one before, and tends to return multiple faces (often in different poses) in the same image. This kind of makes sense, since there are many different types of faces that can all cause a face to be detected.

    On the other hand, maximizing the second last convolutional layer responses returns single faces, probably because they learned to be much more selective in the kind of faces they respond to.
  Different runs on the same filter often returns different poses and expressions, of the same face.
  
  Solving instead for a low activation, often still returns faces, but they aren’t in the center of the image.
This kind of makes sense because the face detector is trained to give a low response if the face has an offset to the window center, treating it as a false positive. So many of the “negative” responses have meanings - They aren’t just random background patterns, they are faces, and face parts, in different areas of the image.


Images that maximize the activation
These are selected filters. Some filters did not correspond to faces, or had multiple faces.
For each filter, there were 900 iterations of gradient ascent, repeated 10 times to create 10 different images.

         



         



         



         



         



         



         



         



         



         



         



         



         



         



         



         



         

Images that minimize the activation</summary></entry><entry><title type="html">Accelerating Deep Neural Networks with Tensor Decompositions</title><link href="http://localhost:4000/deeplearning/tensor-decompositions-deep-learning" rel="alternate" type="text/html" title="Accelerating Deep Neural Networks with Tensor Decompositions" /><published>2018-01-23T20:10:33+02:00</published><updated>2018-01-23T20:10:33+02:00</updated><id>http://localhost:4000/deeplearning/tensor-decompositions-deep-neural-networks</id><content type="html" xml:base="http://localhost:4000/deeplearning/tensor-decompositions-deep-learning">&lt;p&gt;&lt;a href=&quot;https://github.com/jacobgil/pytorch-tensor-decompositions&quot;&gt;My PyTorch implementation for tensor decomposition methods on convolutional layers.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/JeanKossaifi/tensorly-notebooks/blob/master/05_pytorch_backend/cnn_acceleration_tensorly_and_pytorch.ipynb&quot;&gt;Notebook contributed to TensorLy.&lt;/a&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;p&gt;In this post I will cover a few low rank tensor decomposition methods for taking layers in existing deep learning models and making them more compact. I will also share PyTorch code that uses &lt;a href=&quot;https://tensorly.github.io/stable/index.html&quot;&gt;Tensorly&lt;/a&gt; for performing CP decomposition and Tucker decomposition of convolutional layers.&lt;/p&gt;

&lt;p&gt;Although hopefully most of the post is self contained, a good review of tensor decompositions can be found &lt;a href=&quot;http://www.sandia.gov/~tgkolda/pubs/pubfiles/TensorReview.pdf&quot;&gt;here&lt;/a&gt;.
The author of Tensorly also created some &lt;a href=&quot;https://github.com/JeanKossaifi/tensorly-notebooks&quot;&gt;really nice notebooks&lt;/a&gt; about Tensors basics. That helped me getting started, and I recommend going through that.&lt;/p&gt;

&lt;p&gt;Together with pruning, tensor decompositions are practical tools for speeding up existing deep neural networks, and I hope this post will make them a bit more accessible.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;These methods take a layer and decompose it into several smaller layers.  Although there will be more layers after the decomposition, the total number of floating point operations and weights will be smaller.
Some reported results are on the order of x8 for entire networks (not aimed at large tasks like imagenet, though), or x4 for specific layers inside imagenet. 
My experience was that with these decompositions I was able to get a speedup of between x2 to x4, depending on the accuracy drop I was willing to take.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://jacobgil.github.io/deeplearning/pruning-deep-learning&quot;&gt;this blog post&lt;/a&gt; I covered a technique called pruning for reducing the number of parameters in a model. Pruning requires making a forward pass (and sometimes a backward pass) on a dataset, and then ranks the neurons according to some criterion on the activations in the network.&lt;/p&gt;

&lt;p&gt;Quite different from that, tensor decomposition methods use only the weights of a layer, with the assumption that the layer is over parameterized and its weights can be represented by a matrix or tensor with a lower rank.
This means they work best in cases of over parameterized networks. Networks like VGG are over parameterized by design. Another example of an over parameterized model is fine tuning a network for an easier task with fewer categories.&lt;/p&gt;

&lt;p&gt;Similarly to pruning, after the decomposition usually the model needs to be fine tuned to restore accuracy.&lt;/p&gt;

&lt;p&gt;One last thing worth noting before we dive into details, is that while these methods are practical and give nice results, they have a few drawbacks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They operate on the weights of a linear layer (like a convolution or a fully connected layer), and ignore any non linearity that comes after them.&lt;/li&gt;
  &lt;li&gt;They are greedy and perform the decomposition layer wise, ignoring interactions between different layers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are works that try to address these issues, and its still an active research area.&lt;/p&gt;

&lt;h1 id=&quot;truncated-svd-for-decomposing-fully-connected-layers&quot;&gt;Truncated SVD for decomposing fully connected layers&lt;/h1&gt;

&lt;p&gt;The first reference I could find of using this for accelerating deep neural networks, is in the &lt;a href=&quot;https://arxiv.org/abs/1504.08083&quot;&gt;Fast-RCNN&lt;/a&gt; paper. Ross Girshick used it to speed up the fully connected layers used for detection.
Code for this can be found in the &lt;a href=&quot;https://github.com/rbgirshick/py-faster-rcnn/blob/master/tools/compress_net.py&quot;&gt;pyfaster-rcnn implementation.&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;svd-recap&quot;&gt;SVD recap&lt;/h1&gt;
&lt;p&gt;The singular value decomposition lets us decompose any matrix A with n rows and m columns:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A_{nxm} = U_{nxn} S_{nxm} V^T_{mxm}&lt;/script&gt;

&lt;p&gt;S is a diagonal matrix with non negative values along its diagonal (the singular values), and is usually constructed such that the singular values are sorted in descending order.
U and V are orthogonal matrices: &lt;script type=&quot;math/tex&quot;&gt;U^TU=V^TV=I&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;If we take the largest t singular values and zero out the rest, we get an approximation of A: 
&lt;script type=&quot;math/tex&quot;&gt;\hat{A} = U_{nxt}S_{txt}V^T_{mxt}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\hat{A}&lt;/script&gt; has the nice property of being the rank t matrix that has the Frobenius-norm closest to A, so &lt;script type=&quot;math/tex&quot;&gt;\hat{A}&lt;/script&gt; is a good approximation of A if t is large enough.&lt;/p&gt;

&lt;h1 id=&quot;svd-on-a-fully-connected-layer&quot;&gt;SVD on a fully connected layer&lt;/h1&gt;
&lt;p&gt;A fully connected layer essentially does matrix multiplication of its input by a matrix A, and then adds a bias b:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Ax+b&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We can take the SVD of A, and keep only the first t singular values.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;(U_{nxt}S_{txt}V^T_{mxt})x + b&lt;/script&gt; = &lt;script type=&quot;math/tex&quot;&gt;U_{nxt} ( S_{txt}V^T_{mxt} x ) + b&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Instead of a single fully connected layer, this guides us how to implement it as two smaller ones:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first one will have a shape of mxt, will have no bias, and its weights will be taken from &lt;script type=&quot;math/tex&quot;&gt;S_{txt}V^T&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The second one will have a shape of txn, will have a bias equal to b, and its weights will be taken from &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The total number of weights dropped from nxm to t(n+m).&lt;/p&gt;

&lt;h1 id=&quot;tensor-decompositions-on-convolutional-layers&quot;&gt;Tensor decompositions on convolutional layers&lt;/h1&gt;

&lt;p&gt;A 2D convolutional layer is a multi dimensional matrix (from now on - tensor) with 4 dimensions:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cols x rows x input_channels x output_channels&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Following the SVD example, we would want to somehow decompose the tensor into several smaller tensors. The convolutional layer would then be approximated by several smaller convolutional layers.&lt;/p&gt;

&lt;p&gt;For this we will use the two popular (well, at least in the world of Tensor algorithms) tensor decompositions: the CP decomposition and the Tucker decomposition (also called higher-order SVD and many other names).&lt;/p&gt;

&lt;h1 id=&quot;14126553-speeding-up-convolutional-neural-networks-using-fine-tuned-cp-decomposition&quot;&gt;1412.6553 Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.6553&quot;&gt;&lt;em&gt;1412.6553 Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition&lt;/em&gt;&lt;/a&gt; shows how CP-Decomposition can be used to speed up convolutional layers. 
As we will see, this factors the convolutional layer into something that resembles mobile nets.&lt;/p&gt;

&lt;p&gt;They were able to use this to accelerate a network by more than x8 without significant decrease in accuracy. In my own experiments I was able to use this get a x2 speedup on a network based 
on VGG16 without accuracy drop.&lt;/p&gt;

&lt;p&gt;My experience with this method is that the finetuning learning rate needs to be chosen very carefuly to get it to work, and the learning rate should usually be very small (around &lt;script type=&quot;math/tex&quot;&gt;10^{-6}&lt;/script&gt;).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A rank R matrix can be viewed as a sum of R rank 1 matrices, were each rank 1 matrix is a column vector multiplying a row vector: &lt;script type=&quot;math/tex&quot;&gt;\sum_1^Ra_i*b_i^T&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The SVD gives us a way for writing this sum for matrices using the columns of  U and V from the SVD: 
&lt;script type=&quot;math/tex&quot;&gt;\sum_1^R \sigma_i u_i*v_i^T&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If we choose an R that is less than the full rank of the matrix, than this sum is just an approximation, like in the case of truncated SVD.&lt;/p&gt;

&lt;p&gt;The CP decomposition lets us generalize this for tensors.&lt;/p&gt;

&lt;p&gt;Using CP-Decompoisition, our convolutional kernel, a 4 dimensional tensor &lt;script type=&quot;math/tex&quot;&gt;K(i, j, s, t)&lt;/script&gt; can be approximated similarly for a chosen R:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\sum_{r=1}^R K^x_r(i)K^y_r(j)K^s_r(s)K^t_r(t)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We will want R to be small for the decomposition to be effecient, but large enough to keep a high approximation accuracy.&lt;/p&gt;

&lt;h1 id=&quot;the-convolution-forward-pass-with-cp-decomposition&quot;&gt;The convolution forward pass with CP Decomposition&lt;/h1&gt;
&lt;p&gt;To forward the layer, we do convolution with an input &lt;script type=&quot;math/tex&quot;&gt;X(i, j, s)&lt;/script&gt;:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;V(x, y, t) = \sum_i \sum_j \sum_sK(i, j, s, t)X(x-i, y-j, s)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \sum_r\sum_i \sum_j \sum_sK^x_r(i)K^y_r(i)K^s_r(s)K^t_r(t)X(x-i, y-j, s)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \sum_rK^t_r(t) \sum_i \sum_j K^x_r(i)K^y_r(i)\sum_sK^s_r(s)X(x-i, y-j, s)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This gives us a recipe to do the convlution:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;First do a point wise (1x1xS) convolution with &lt;script type=&quot;math/tex&quot;&gt;K_r(s)&lt;/script&gt;. 
 This reduces the number of input channels from S to R.
 The convolutions will next be done on a smaller number of channels, making them faster.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Perform seperable convolutions in the spatial dimensions with &lt;script type=&quot;math/tex&quot;&gt;K^x_r,K^y_r&lt;/script&gt;.
&lt;strong&gt;Like in &lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;&gt;mobilenets&lt;/a&gt; the convolutions are depthwise seperable, done in each channel separately.&lt;/strong&gt;
&lt;strong&gt;Unlike mobilenets the convolutions are also separable in the spatial dimensions.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Do another pointwise convolution to change the number of channels from R to T
If the original convolutional layer had a bias, add it at this point.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Notice the combination of pointwise and depthwise convolutions like in mobilenets. While with mobilenets you have to train a network from scratch to get this structure, here we can decompose an existing layer into this form.&lt;/p&gt;

&lt;p&gt;As with mobile nets, to get the most speedup you will need a platform that has an efficient implementation of depthwise separable convolutions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/62e348e26976c3ef77909b9af9788ebc2509009a/3-Figure1-1.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Image taken from the &lt;a href=&quot;https://arxiv.org/abs/1412.6553&quot;&gt;paper&lt;/a&gt;. The bottom row is an illustration of the convolution steps after CP-decomposition.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;convolutional-layer-cp-decomposition-with-pytorch-and-tensorly&quot;&gt;Convolutional layer CP-Decomposition with PyTorch and Tensorly&lt;/h1&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cp_decomposition_conv_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Gets a conv layer and a target rank, 
        returns a nn.Sequential object with the decomposition &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Perform CP decomposition on the layer weight tensorly. &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;horizontal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
        &lt;span class=&quot;n&quot;&gt;parafac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'svd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;pointwise_s_to_r_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;depthwise_vertical_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertical&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertical&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertical&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertical&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;depthwise_horizontal_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
        &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;groups&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;pointwise_r_to_t_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;pointwise_r_to_t_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;depthwise_horizontal_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
        &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;horizontal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;depthwise_vertical_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
        &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pointwise_s_to_r_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
        &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pointwise_r_to_t_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;new_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pointwise_s_to_r_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;depthwise_vertical_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; \
                    &lt;span class=&quot;n&quot;&gt;depthwise_horizontal_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pointwise_r_to_t_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;151106530-compression-of-deep-convolutional-neural-networks-for-fast-and-low-power-mobile-applications&quot;&gt;1511.06530 Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06530&quot;&gt;&lt;em&gt;1511.06530 Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications&lt;/em&gt;&lt;/a&gt; is a really cool paper that shows how to use the Tucker Decomposition for speeding up convolutional layers with even better results.
I also used this accelerate an over-parameterized VGG based network, with better accuracy than CP Decomposition. As the authors note in the paper, it lets us do the finetuning using higher learning rates (I used &lt;script type=&quot;math/tex&quot;&gt;10^{-3}&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;The Tucker Decomposition, also known as the higher order SVD (HOSVD) and many other names, is a generalization of SVD for tensors. 
&lt;script type=&quot;math/tex&quot;&gt;K(i, j, s, t) = \sum_{r_1=1}^{R_1}\sum_{r_2=1}^{R_2}\sum_{r_3=1}^{R_3}\sum_{r_4=1}^{R_4}\sigma_{r_1 r_2 r_3 r_4} K^x_{r1}(i)K^y_{r2}(j)K^s_{r3}(s)K^t_{r4}(t)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The reason its considered a generalization of the SVD is that often the components of &lt;script type=&quot;math/tex&quot;&gt;\sigma_{r_1 r_2 r_3 r_4}&lt;/script&gt; are orthogonal, but this isn’t really important for our purpose.
&lt;script type=&quot;math/tex&quot;&gt;\sigma_{r_1 r_2 r_3 r_4}&lt;/script&gt; is called the core matrix, and defines how different axis interact.&lt;/p&gt;

&lt;p&gt;In the CP Decomposition described above, the decomposition along the spatial dimensions  &lt;script type=&quot;math/tex&quot;&gt;K^x_r(i)K^y_r(j)&lt;/script&gt; caused a spatially separable convolution. The filters are quite small anyway, typically 3x3 or 5x5, so the separable convolution isn’t saving us a lot of computation, and is an aggressive approximation.&lt;/p&gt;

&lt;p&gt;The Tucker decomposition has the useful property that it doesn’t have to be decomposed along all the axis (modes).
We can perform the decomposition along the input and output channels instead (a mode-2 decomposition):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(i, j, s, t) = \sum_{r_3=1}^{R_3}\sum_{r_4=1}^{R_4}\sigma_{i j r_3 r_4}(j)K^s_{r3}(s)K^t_{r4}(t)&lt;/script&gt;

&lt;h1 id=&quot;the-convolution-forward-pass-with-tucker-decomposition&quot;&gt;The convolution forward pass with Tucker Decomposition&lt;/h1&gt;

&lt;p&gt;Like for CP decomposition, lets write the convolution formula and plug in the kernel decomposition:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(x, y, t) = \sum_i \sum_j \sum_sK(i, j, s, t)X(x-i, y-j, s)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(x, y, t) = \sum_i \sum_j \sum_s\sum_{r_3=1}^{R_3}\sum_{r_4=1}^{R_4}\sigma_{(i)(j) r_3 r_4}K^s_{r3}(s)K^t_{r4}(t)X(x-i, y-j, s)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(x, y, t) = \sum_i \sum_j \sum_{r_4=1}^{R_4}\sum_{r_3=1}^{R_3}K^t_{r4}(t)\sigma_{(i)(j) r_3 r_4} \sum_s\ K^s_{r3}(s)X(x-i, y-j, s)&lt;/script&gt;

&lt;p&gt;This gives us the following recipe for doing the convolution with Tucker Decomposition:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Point wise convolution with &lt;script type=&quot;math/tex&quot;&gt;K^s_{r3}(s)&lt;/script&gt; for reducing the number of channels from S to &lt;script type=&quot;math/tex&quot;&gt;R_3&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Regular (not separable) convolution with &lt;script type=&quot;math/tex&quot;&gt;\sigma_{(i)(j) r_3 r_4}&lt;/script&gt;.
 Instead of S input channels and T output channels like the original layer had,
 this convolution has &lt;script type=&quot;math/tex&quot;&gt;R_3&lt;/script&gt; input channels and &lt;script type=&quot;math/tex&quot;&gt;R_4&lt;/script&gt; output channels.  If these ranks are smaller than S and T, this is were the reduction comes from.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pointwise convolution with &lt;script type=&quot;math/tex&quot;&gt;K^t_{r4}(t)&lt;/script&gt; to get back to T output channels like the original convolution.
 Since this is the last convolution, at this point we add the bias if there is one.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;how-can-we-select-the-ranks-for-the-decomposition-&quot;&gt;How can we select the ranks for the decomposition ?&lt;/h1&gt;
&lt;p&gt;One way would be trying different values and checking the accuracy. I played with heuristics like &lt;script type=&quot;math/tex&quot;&gt;R_3 = S/3&lt;/script&gt; , &lt;script type=&quot;math/tex&quot;&gt;R_4 = T/3&lt;/script&gt; with good results.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ideally selecting the ranks should be automated.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The authors suggested using &lt;a href=&quot;http://www.jmlr.org/papers/volume14/nakajima13a/nakajima13a.pdf&quot;&gt;variational Bayesian matrix factorization (VBMF) (Nakajima et al., 2013)&lt;/a&gt; as a method  for estimating the rank.&lt;/p&gt;

&lt;p&gt;VBMF is complicated and is out of the scope of this post, but in a really high level summary what they do is approximate a matrix &lt;script type=&quot;math/tex&quot;&gt;V_{LxM}&lt;/script&gt; as the sum of a lower ranking matrix &lt;script type=&quot;math/tex&quot;&gt;B_{LxH}A^T_{HxM}&lt;/script&gt; and gaussian noise. 
After A and B are found, H is an upper bound on the rank.&lt;/p&gt;

&lt;p&gt;To use this for tucker decomposition, we can unfold the s and t components of the original weight tensor to create matrices. Then we can estimate &lt;script type=&quot;math/tex&quot;&gt;R_3&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;R_4&lt;/script&gt; as the rank of the matrices using VBMF.&lt;/p&gt;

&lt;p&gt;I used this &lt;a href=&quot;https://github.com/CasvandenBogaard/VBMF&quot;&gt;python implementation of VBMF&lt;/a&gt; and got convinced it works :-)&lt;/p&gt;

&lt;p&gt;VBMF usually returned ranks very close to what I previously found with careful and tedious manual tuning.&lt;/p&gt;

&lt;p&gt;This could also be used for estimating the rank for Truncated SVD acceleration of fully connected layers.&lt;/p&gt;

&lt;h1 id=&quot;convolutional-layer-tucker-decomposition-with-pytorch-and-tensorly&quot;&gt;Convolutional layer Tucker-Decomposition with PyTorch and Tensorly&lt;/h1&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;estimate_ranks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Unfold the 2 modes of the Tensor the decomposition will 
    be performed on, and estimates the ranks of the matrices using VBMF 
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;unfold_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorly&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unfold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;unfold_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorly&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unfold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diag_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VBMF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EVBMF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unfold_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diag_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VBMF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EVBMF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unfold_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ranks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag_0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diag_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ranks&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tucker_decomposition_conv_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Gets a conv layer, 
        returns a nn.Sequential object with the Tucker decomposition.
        The ranks are estimated with a Python implementation of VBMF
        https://github.com/CasvandenBogaard/VBMF
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;ranks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_ranks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;VBMF Estimated ranks&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ranks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;core&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
        &lt;span class=&quot;n&quot;&gt;partial_tucker&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;modes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ranks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ranks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'svd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# A pointwise convolution that reduces the channels from S to R3&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;first_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# A regular 2D convolution layer with R3 input channels &lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# and R3 output channels&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;core_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;core&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# A pointwise convolution that increases the channels from R4 to T&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;last_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; \
        &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dilation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;last_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;first_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
        &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;last_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;core_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;core&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;new_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;first_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;core_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post we went over a few tensor decomposition methods for accelerating layers in deep neural networks.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Truncated SVD can be used for accelerating fully connected layers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CP Decomposition decomposes convolutional layers into something that resembles mobile-nets, although it is even more aggressive since it is also separable in the spatial dimensions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tucker Decomposition reduced the number of input and output channels the 2D convolution layer operated on, and used pointwise convolutions to switch the number of channels before and after the 2D convolution.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I think it’s interesting how common patterns in network design, pointwise and depthwise convolutions, naturally appear in these decompositions!&lt;/p&gt;</content><summary type="html">My PyTorch implementation for tensor decomposition methods on convolutional layers.

Notebook contributed to TensorLy.</summary></entry><entry><title type="html">Pruning deep neural networks to make them fast and small</title><link href="http://localhost:4000/deeplearning/pruning-deep-learning" rel="alternate" type="text/html" title="Pruning deep neural networks to make them fast and small" /><published>2017-06-23T15:10:33+03:00</published><updated>2017-06-23T15:10:33+03:00</updated><id>http://localhost:4000/deeplearning/pruning-deep-neural-networks</id><content type="html" xml:base="http://localhost:4000/deeplearning/pruning-deep-learning">&lt;p&gt;&lt;a href=&quot;https://github.com/jacobgil/pytorch-pruning&quot;&gt;My PyTorch implementation&lt;/a&gt; of &lt;a href=&quot;https://arxiv.org/abs/1611.06440&quot;&gt;[1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference]&lt;/a&gt;.
&lt;!--more--&gt;
TL;DR: By using pruning a VGG-16 based Dogs-vs-Cats classifier is made x3 faster and x4 smaller.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Pruning neural networks is an old idea going back to 1990 &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf&quot;&gt;(with Yan Lecun’s optimal brain damage work)&lt;/a&gt; and before.
The idea is that among the many parameters in the network, some are redundant and don’t contribute a lot to the output.&lt;/p&gt;

&lt;p&gt;If you could rank the neurons in the network according to how much they contribute, you could then remove the low ranking neurons from the network, resulting in a smaller and faster network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Getting faster/smaller networks is important for running these deep learning networks on mobile devices.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The ranking can be done according to the L1/L2 mean of neuron weights, their mean activations, the number of times a neuron wasn’t zero on some validation set, and other creative methods .
After the pruning, the accuracy will drop (hopefully not too much if the ranking clever), and the network is usually trained more to recover.&lt;/p&gt;

&lt;p&gt;If we prune too much at once, the network might be damaged so much it won’t be able to recover.&lt;/p&gt;

&lt;p&gt;So in practice this is an iterative process - often called ‘Iterative Pruning’: Prune / Train / Repeat.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/pruning_steps.png&quot; alt=&quot;Pruning steps&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The image is taken from &lt;a href=&quot;https://arxiv.org/abs/1611.06440&quot;&gt;[1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;sounds-good-why-isnt-this-more-popular&quot;&gt;Sounds good, why isn’t this more popular?&lt;/h2&gt;

&lt;p&gt;There are a lot of papers about pruning, but I’ve never encountered pruning used in real life deep learning projects.&lt;/p&gt;

&lt;p&gt;Which is surprising considering all the effort on running deep learning on mobile devices.
I guess the reason is a combination of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The ranking methods weren’t good enough until now, resulting in too big of an accuracy drop.&lt;/li&gt;
  &lt;li&gt;It’s a pain to implement.&lt;/li&gt;
  &lt;li&gt;Those who do use pruning, keep it for themselves as a secret sauce advantage.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, I decided to implement pruning myself and see if I could get good results with it.&lt;/p&gt;

&lt;p&gt;In this post we will go over a few pruning methods, and then dive into the implementation details of one of the recent methods.&lt;/p&gt;

&lt;p&gt;We will fine tune a VGG network to classify cats/dogs on the &lt;a href=&quot;https://www.kaggle.com/c/dogs-vs-cats&quot;&gt;Kaggle Dogs vs Cats dataset&lt;/a&gt;, which represents a kind of transfer learning that I think is very common in practice.&lt;/p&gt;

&lt;p&gt;Then we will prune the network and speed it up by a factor of almost x3, and reduce the size by a factor of almost x4!&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;pruning-for-speed-vs-pruning-for-a-small-model&quot;&gt;Pruning for speed vs Pruning for a small model&lt;/h2&gt;

&lt;p&gt;In VGG16 90% of the weights are in the fully connected layers, but those account for 1% of the total floating point operations.&lt;/p&gt;

&lt;p&gt;Up until recently most of the works focused on pruning the fully connected layers. By pruning those, the model size can be dramatically reduced.&lt;/p&gt;

&lt;p&gt;We will focus here on pruning entire filters in convolutional layers.&lt;/p&gt;

&lt;p&gt;But this has a cool side affect of also reducing memory. As observed in the &lt;a href=&quot;https://arxiv.org/abs/1611.06440&quot;&gt;[1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference]&lt;/a&gt; paper, the deeper the layer, the more it will get pruned.&lt;/p&gt;

&lt;p&gt;This means the last convolutional layer will get pruned a lot, and a lot of neurons from the fully connected layer following it will also be discarded!&lt;/p&gt;

&lt;p&gt;When pruning the convolutional filters, another option would be to reduce the weights in each filter, or remove a specific dimension of a single kernel. You can end up with filters that are sparse,
but it’s not trivial the get a computational speed up. Recent works advocate “Structured sparsity” where entire filters are pruned instead.&lt;/p&gt;

&lt;p&gt;One important thing several of these papers show, is that by training and then pruning a larger network, especially in the case of transfer learning, they get results that are much better than training a smaller network from scratch.&lt;/p&gt;

&lt;p&gt;Lets now briefly review a few methods.&lt;/p&gt;

&lt;h2 id=&quot;160808710-pruning-filters-for-effecient-convnets&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.08710&quot;&gt;[1608.08710 Pruning filters for effecient convnets]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In this work they advocate pruning entire convolutional filters.
Pruning a filter with index k affects the layer it resides in, and the following layer.
All the input channels at index k, in the following layer, will have to removed, since they won’t exist any more after the pruning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prune_example.png&quot; alt=&quot;Pruning a convolutional filter entire filter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The image is from &lt;a href=&quot;https://arxiv.org/abs/1608.08710&quot;&gt;[1608.08710 Pruning filters for effecient convnets]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In case the following layer is a fully connected layer, and the size of the feature map of that channel would be MxN, then MxN neurons be removed from the fully connected layer.&lt;/p&gt;

&lt;p&gt;The neuron ranking in this work is fairly simple. It’s the L1 norm of the weights of each filter.&lt;/p&gt;

&lt;p&gt;At each pruning iteration they rank all the filters, prune the m lowest ranking filters globally among all the layers, retrain and repeat.&lt;/p&gt;

&lt;h2 id=&quot;151208571-structured-pruning-of-deep-convolutional-neural-networks&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.08571&quot;&gt;[1512.08571 Structured Pruning of Deep Convolutional Neural Networks]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This work seems similar, but the ranking is much more complex.
They keep a set of N particle filters, which represent N convolutional filters to be pruned.&lt;/p&gt;

&lt;p&gt;Each particle is assigned a score based on the network accuracy on a validation set, when the filter represented by the particle was not masked out. Then based on the new score, new pruning masks are sampled.&lt;/p&gt;

&lt;p&gt;Since running this process is heavy, they used a small validation set for measuring the particle scores.&lt;/p&gt;

&lt;h2 id=&quot;161106440-pruning-convolutional-neural-networks-for-resource-efficient-inference&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.06440&quot;&gt;[1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference]&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a really cool work from Nvidia.&lt;/p&gt;

&lt;p&gt;First they state the pruning problem as a combinatorial optimization problem: choose a subset of weights B, such that when pruning them the network cost change will be minimal.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prune_equation.png&quot; alt=&quot;Pruning as a combinatorial optimization problem&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how they used the absolute difference and not just the difference. 
Using the absolute difference enforces that the pruned network won’t decrease the network performance too much, but it also shouldn’t increase it. In the paper they show this gives better results, presumably because it’s more stable.&lt;/p&gt;

&lt;p&gt;Now all ranking methods can be judged by this cost function.&lt;/p&gt;

&lt;h2 id=&quot;oracle-pruning&quot;&gt;Oracle pruning&lt;/h2&gt;

&lt;p&gt;VGG16 has 4224 convolutional filters. The “ideal” ranking method would be brute force - prune each filter, and then observe how the cost function changes when running on the training set.
Since they are from Nvidia and they have access to a gazillion GPUs they did just that.
This is called the oracle ranking - the best possible ranking for minimizing the network cost change.
Now to measure the effectiveness of other ranking methods, they compute the spearman correlation with the oracle. Surprise surprise, the ranking method they came up with (described next) correlates most with the oracle.&lt;/p&gt;

&lt;p&gt;They come up with a new neuron ranking method based on a first order (meaning fast to compute) Taylor expansion of the network cost function.&lt;/p&gt;

&lt;p&gt;Pruning a filter h is the same as zeroing it out.&lt;/p&gt;

&lt;p&gt;C(W, D) is the average network cost function on the dataset D, when the network weights are set to W. Now we can evaluate C(W, D) as an expansion around C(W, D, h = 0).
They should be pretty close, since removing a single filter shouldn’t affect the cost too much.&lt;/p&gt;

&lt;p&gt;The ranking of h is then abs(C(W, D, h = 0) - C(W, D)).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prune_taylor_equation_1.png&quot; alt=&quot;Taylor expansion&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/assets/prune_taylor_equation_2.png&quot; alt=&quot;Taylor expansion&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The rankings of each layer are then normalized by the L2 norm of the ranks in that layer. I guess this kind of empiric, and i’m not sure why is this needed, but it greatly effects the quality of the pruning.&lt;/p&gt;

&lt;p&gt;This rank is quite intuitive. We could’ve used both the activation, and the gradient, as ranking methods by themselves. If any of them are high, that means they are significant to the output. Multiplying them gives us a way to throw/keep the filter if either the gradients or the activations are very low or high.&lt;/p&gt;

&lt;p&gt;This makes me wonder - did they pose the pruning problem as minimizing the difference of the network costs, and &lt;strong&gt;then&lt;/strong&gt; come up with the taylor expansion method,
or was it &lt;strong&gt;other way around&lt;/strong&gt;, and the difference of network costs oracle was a way to back up their new method  ?   :-)&lt;/p&gt;

&lt;p&gt;In the paper their method outperformed other methods in accuracy, too, so it looks like the oracle is a good indicator.&lt;/p&gt;

&lt;p&gt;Anyway I think this is a nice method that’s more friendly to code and test, than say, a particle filter, so we will explore this further!&lt;/p&gt;

&lt;h1 id=&quot;pruning-a-cats-vs-dogs-classifier-using-the-taylor-criteria-ranking&quot;&gt;Pruning a Cats vs Dogs classifier using the Taylor criteria ranking&lt;/h1&gt;

&lt;p&gt;So lets say we have a transfer learning task where we need to create a classifier from a relatively small dataset. 
&lt;a href=&quot;https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html&quot;&gt;Like in this Keras blog post.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Can we use a powerful pre-trained network like VGG for transfer learning, and then prune the network?&lt;/p&gt;

&lt;p&gt;If many features learned in VGG16 are about cars, peoples and houses - how much do they contribute to a simple dog/cat classifier ?&lt;/p&gt;

&lt;p&gt;This is a kind of a problem that I think is very common.&lt;/p&gt;

&lt;p&gt;As a training set we will use 1000 images of cats, and 1000 images of dogs, from the &lt;a href=&quot;%5BKaggle%20Dogs%20vs%20Cats%20dataset%5D%28https://www.kaggle.com/c/dogs-vs-cats%29&quot;&gt;Kaggle Dogs vs Cats data set&lt;/a&gt;.
As a testing set we will use 400 images of cats, and 400 images of dogs.&lt;/p&gt;

&lt;h2 id=&quot;first-lets-show-off-some-statistics&quot;&gt;First lets show off some statistics.&lt;/h2&gt;

&lt;p&gt;The accuracy dropped from 98.7% to 97.5%.&lt;/p&gt;

&lt;p&gt;The network size reduced from 538 MB to 150 MB.&lt;/p&gt;

&lt;p&gt;On a i7 CPU the inference time reduced from 0.78 to 0.277 seconds for a single image, &lt;strong&gt;almost a factor x3 reduction!&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-one---train-a-large-network&quot;&gt;Step one - train a large network&lt;/h2&gt;

&lt;p&gt;We will take VGG16, drop the fully connected layers, and add three new fully connected layers.
We will freeze the convolutional layers, and retrain only the new fully connected layers.
In PyTorch, the new layers look like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25088&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;After training for 20 epoches with data augmentation, we get an accuracy of 98.7% on the testing set.&lt;/p&gt;

&lt;h2 id=&quot;step-two---rank-the-filters&quot;&gt;Step two - Rank the filters&lt;/h2&gt;
&lt;p&gt;To compute the Taylor criteria, we need to perform a Forward+Backward pass on our dataset (or on a smaller part of it if it’s too large. but since we have only 2000 images lets use that).&lt;/p&gt;

&lt;p&gt;Now we need to somehow get both the gradients and the activations for convolutional layers. In PyTorch we can register a hook on the gradient computation, so a callback is called when they are ready:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_modules&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;register_hook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compute_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation_to_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;activation_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now we have the activations in self.activations, and when a gradient is ready, compute_rank will be called:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compute_rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;activation_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
		&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;\
			&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
	
	&lt;span class=&quot;c&quot;&gt;# Normalize the rank by the filter dimensions&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
		&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_index&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter_ranks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter_ranks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
			&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FloatTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter_ranks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
	&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This did a point wise multiplication of each activation in the batch and it’s gradient, and then for each activation (that is an output of a convolution) we sum in all dimensions except the dimension of the output.&lt;/p&gt;

&lt;p&gt;For example, if the batch size was 32, the number of outputs for a specific activation was 256 and the spatial size of that activation was 112x112 such the activation/gradient shapes were 32x256x112x112, then the output will be a 256 sized vector representing the ranks of the 256 filters in this layer.&lt;/p&gt;

&lt;p&gt;Now that we have the ranking, we can use a min heap to get the N lowest ranking filters. Unlike in the Nvidia paper where they used N=1 at each iteration, to get results faster we will use N=512! This means that each pruning iteration, we will remove 12% from the original number of the 4224 convolutional filters.&lt;/p&gt;

&lt;p&gt;The distribution of the low ranking filters is interesting.
Most of the filters pruned are from the deeper layer.
Here is a peek of which filters were pruned after the first iteration:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Layer number&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Number of pruned filters pruned&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 0&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 7&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 10&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 12&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 14&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 17&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;51&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 19&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 21&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;52&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 24&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;68&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 26&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;74&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Layer 28&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;73&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;step-3---fine-tune-and-repeat&quot;&gt;Step 3 - Fine tune and repeat&lt;/h2&gt;
&lt;p&gt;At this stage, we unfreeze all the layers and retrain the network for 10 epoches, which was enough to get good results on this dataset.
Then we go back to step 1 with the modified network, and repeat.&lt;/p&gt;

&lt;p&gt;This is the real price we pay - that’s 50% of the number of epoches used to train the network, at a single iteration. In this toy dataset we can get away with it since the dataset is small.
If you’re doing this for a huge dataset, you better have lots of GPUs.&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;I think pruning is an overlooked method that is going to get a lot more attention and use in practice.
We showed how we can get nice results on a toy dataset. 
I think many problems deep learning is used to solve in practice are similar to this one, using transfer learning on a limited dataset, so they can benefit from pruning too.&lt;/p&gt;</content><summary type="html">My PyTorch implementation of [1611.06440 Pruning Convolutional Neural Networks for Resource Efficient Inference].</summary></entry><entry><title type="html">Visualizations for regressing wheel steering angles in self driving cars</title><link href="http://localhost:4000/deeplearning/vehicle-steering-angle-visualizations" rel="alternate" type="text/html" title="Visualizations for regressing wheel steering angles in self driving cars" /><published>2016-10-26T23:10:33+03:00</published><updated>2016-10-26T23:10:33+03:00</updated><id>http://localhost:4000/deeplearning/visualizations-for-steering-angle-regression-keras</id><content type="html" xml:base="http://localhost:4000/deeplearning/vehicle-steering-angle-visualizations">&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/18123_cam.jpg?raw=true&quot; alt=&quot;grad-cam&quot; /&gt;
&lt;!--more--&gt;
&lt;em&gt;Pixels that contribute to steering right, using the grad-cam method described below&lt;/em&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/w6XHI1oIbOQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;em&gt;Video using the hypercolumns and occlusion map methods described below&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations&quot;&gt;Github repo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;This post is about understanding how a self driving deep learning network decides to steer the car wheel.&lt;/p&gt;

&lt;p&gt;NVIDIA published a &lt;a href=&quot;https://arxiv.org/pdf/1604.07316.pdf&quot;&gt;very interesting paper,&lt;/a&gt; that describes how a deep learning network can be trained to steer a wheel, given a 200x66 RGB image from the front of a car.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/SullyChen/Nvidia-Autopilot-TensorFlow&quot;&gt;This repository&lt;/a&gt; shared a Tensorflow implementation of the network described in the paper, and (thankfully!) a dataset of image / steering angles collected from a human driving a car.
The dataset is quite small, and there are much larger datasets available like in the &lt;a href=&quot;https://medium.com/udacity/challenge-2-using-deep-learning-to-predict-steering-angles-f42004a36ff3#.cdis1phrk&quot;&gt;udacity challenge&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However it is great for quickly experimenting with these kind of networks, and visualizing when the network is overfitting is also interesting.
I ported the code to Keras, trained a (very over-fitting) network based on the NVIDIA paper, and made visualizations.&lt;/p&gt;

&lt;p&gt;I think that if eventually this kind of a network will find use in a real world self driving car, being able to debug it and understand its output will be crucial.&lt;/p&gt;

&lt;p&gt;Otherwise the first time the network decides to make a very wrong turn, critics will say that this is just a black box we don’t understand, and it should be replaced!&lt;/p&gt;

&lt;h2 id=&quot;first-attempt--treating-the-network-as-a-black-box---occlusion-maps&quot;&gt;First attempt : Treating the network as a black box - occlusion maps&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/25123.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/25123_occlusion.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first thing we will try, won’t require any knowledge about the network, and in fact we won’t peak inside the network, just look at the output.
We”l create an occlusion map for a given image, where we take many windows in the image, mask them out, run the network, and see how the regressed angle changed.
If the angle changed a lot - that window contains information that was important for the network decision.
We then can assign each window a score based on how the angle changed!&lt;/p&gt;

&lt;p&gt;We need to take many windows, with different sizes - since we don’t know in advance the sizes of important features in the image.&lt;/p&gt;

&lt;p&gt;Now we can make nice effects like filtering the occlusion map, and displaying the focused area on top of a blurred image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/10123_occlusion_blurred.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Some problems with this - 
Its expensive to create the visualization since we need many sliding windows,
and it is possible that just masking out the windows created artificial features like sharp angles that were used by the network.
Also - this tells us which areas were important for the network, but it doesn’t give us any insight on why.
Can we do better?&lt;/p&gt;

&lt;h2 id=&quot;second-attempt---peaking-at-the-conv-layers-output-features-with-hypercolumns&quot;&gt;Second attempt - peaking at the conv layers output features with hypercolumns&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/38123_hypercolumns.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/18123_hypercolumns.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So we want to understand what kind of features the network saw in the image, and how it used them for its final decision.
Lets use a heuristic - take the outputs of the convolutional layers, resize them to the input image size, and aggregate them.
The collection of these outputs are called hypercolumns, and &lt;a href=&quot;http://blog.christianperone.com/2016/01/convolutional-hypercolumns-in-python/&quot;&gt;here&lt;/a&gt; is a good blog post about getting them with Keras.
One way of aggregating them is by just multiplying them - so pixels that had high activation in all layers will get a high score.
We will take the average output image from each layer, normalize it, and multiply these values from wanted layers.
In the NVIDIA model, the output from the last convolutional layer is a 18x1 image.
If we peak only at that layer, we basically get a importance map for columns of the image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/18123_hypercolumns_lastlayer.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Anyway, this is quite naive and completely ignores the fully connected layers, and the fact that in certain situations some outputs are much more important than other outputs, but its a heuristic.&lt;/p&gt;

&lt;h2 id=&quot;third-attempt---getting-there---class-activation-maps-using-gradients&quot;&gt;Third attempt - Getting there - class activation maps using gradients&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/18123_cam.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(The above image shows pixels that contribute to steering right)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://jacobgil.github.io/deeplearning/2016/08/19/class-activation-maps.html&quot;&gt;Class activation maps&lt;/a&gt; are a technique to visualize the importance of image pixels to the final output of the network.
Basically you take the output of the last convolutional layer, you take a spatial average of that (global average pooling), and you feed that into a softmax for classification.
Now you can look at the softmax weights used to give a category score - large weights mean important features - and multiply them by the corresponding conv outputs.&lt;/p&gt;

&lt;p&gt;Relative to the rest of the stuff we tried here - this technique is great. It gives us an insight of how exactly each pixel was used in the overall decision process.
However this technique requires a specific network architecture - conv layers + GAP (global average pooling, the spatial mean for every channel in a feature map), so existing networks with fully connected layers, like the nvidia model, can’t be used as is.
We could just train a new model with conv layers + GAP (I actually did that), however we really want the fully connected layers here. They enable the network to reason spatially about the image - If it finds interesting features in the left part of the image - perhaps that road is blocked?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1610.02391v1.pdf&quot;&gt;This paper&lt;/a&gt; solves the issue, and generalizes class activation maps.
To get the importance of images in the conv outputs, you use back propagation - you take the gradient of the target output with respect to the pixels in conv output images.
Conv output images that are important for the final classification decision, will contain a lot of positive gradients. So to assign them an importance value - we can just take a spatial average of the gradients in each conv output image (global average pooling again).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jacobgil/keras-grad-cam&quot;&gt;I wrote some Keras code to try this out for classification networks.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So lets adapt this for the steering angle regression.
We can’t just always take gradient of the output, since now when the gradient is high, it isn’t contributing to a certain category like in the classification case, but instead to a positive steering angle. And maybe the actual steering angle was negative.&lt;/p&gt;

&lt;p&gt;Lets look at the gradient of the regressed angle with respect to some pixel in some output image - 
If the gradient is very positive, that means that the pixel contributes to enlarging the steering angle - steering right.
If the gradient is very negative, the pixel contributes to steering left.
If the gradient is very small, the pixel contributes to not steering at all.&lt;/p&gt;

&lt;p&gt;We can divide the angles into ranges - if the actual output angle was large, we can peak at the image features that contributed to a positive steering angle, etc.
If the angle is small, we will just take the inverse of the steering angle as our target - since then pixels that contribute to small angles will get large gradients.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;grad_cam_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;180.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
	    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scipy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;180.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
	    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Lets look at an example.
For the same image, we could target pixels that contribute to steering right:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/19943_cam_right.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And we could also target pixels that contribute to steering to the center:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/19943_cam_center.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-steering-angle-visualizations/blob/master/examples/1123_cam.jpg?raw=true&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;</content><summary type="html"></summary></entry><entry><title type="html">Class activation maps in Keras for visualizing where deep learning networks pay attention</title><link href="http://localhost:4000/deeplearning/class-activation-maps" rel="alternate" type="text/html" title="Class activation maps in Keras for visualizing where deep learning networks pay attention" /><published>2016-08-19T23:10:33+03:00</published><updated>2016-08-19T23:10:33+03:00</updated><id>http://localhost:4000/deeplearning/class-activation-maps</id><content type="html" xml:base="http://localhost:4000/deeplearning/class-activation-maps">&lt;p&gt;&lt;a href=&quot;https://github.com/jacobgil/keras-cam&quot;&gt;Github project for class activation maps&lt;/a&gt;
&lt;a href=&quot;https://github.com/jacobgil/keras-grad-cam&quot;&gt;Github repo for gradient based class activation maps&lt;/a&gt;
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cnnlocalization.csail.mit.edu&quot;&gt;Class activation maps&lt;/a&gt; are a simple technique to get the discriminative image regions used by a CNN to identify a specific class in the image.
In other words, a class activation map (CAM) lets us see which regions in the image were relevant to this class.
The authors of the paper show that this also allows re-using classifiers for getting good localization results, even when training without bounding box coordinates data.
This also shows how deep learning networks already have some kind of a built in attention mechanism.&lt;/p&gt;

&lt;p&gt;This should be useful for debugging the decision process in classification networks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-cam/blob/master/examples/mona_lisa.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To be able to create a CAM, the network architecture is restricted to have a global average pooling layer after the final convolutional layer, and then a linear (dense) layer. 
Unfortunately this means we can’t apply this technique on existing networks that don’t have this structure. What we can do is modify existing networks and fine tune them to get this. 
Designing network architectures to support tricks like CAM is like writing code in a way that makes it easier to debug.&lt;/p&gt;

&lt;p&gt;The first building block for this is a layer called &lt;em&gt;global average pooling&lt;/em&gt;.
After the last convolutional layer in a typical network like VGG16, we have an N-dimensional image, where N is the number of filters in this layer.
For example in VGG16, the last convolutional layer has 512 filters.
For an 1024x1024 input image (lets discard the fully connected layers, so we can use any input image size we want), the output shape of the last convolutional layer will be 512x64x64.  Since 1024/64 = 16, we have a 16x16 spatial mapping resolution.
A global average pooling (GAP) layer just takes each of these 512 channels, and returns their spatial average.
Channels with high activations, will have high signals.
Lets look at keras code for this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;global_average_pooling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;global_average_pooling_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The output shape of the convolutional layer will be [batch_size, number of filters, width, height].
So we can take the average in the width/height axes (2, 3).
We also need to specify the output shape from the layer, so Keras can do shape inference for the next layers. Since we are creating a custom layer here, Keras doesn’t really have a way to just deduce the output size by itself.&lt;/p&gt;

&lt;p&gt;The second building block is to assign a weight to each output from the global average pooling layer, for each of the categories.
This can be done by adding a dense linear layer + softmax, training an SVM on the GAP output, or applying any other linear classifier on top of the GAP.
These weights set the importance of each of the convolutional layer outputs.&lt;/p&gt;

&lt;p&gt;Lets combine these building blocks in Keras code:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;VGG16_convolutions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;vgg16_weights.h5&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	    
	    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_average_pooling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
	              &lt;span class=&quot;n&quot;&gt;output_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_average_pooling_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'uniform'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;sgd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;momentum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nesterov&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'categorical_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sgd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now to create a heatmap for a class we can just take output images from the last convolutional layer, multiply them by their assigned weights (different weights for each class), and sum.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;visualize_class_activation_map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;original_img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;original_img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;#Reshape to the network input shape (3, w, h).&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;#Get the 512 input weights to the softmax.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;class_weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;final_conv_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_output_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;conv5_3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;get_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; \
                    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_conv_layer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;conv_outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;#Create the class activation map.&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv_outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;target_class&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv_outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To test this out I trained a poor man’s person/not person classifier on person images from here:
http://pascal.inrialpes.fr/data/human
In the training all the images are resized to 68x128, and 20% of the images are used for validation. 
After 11 epochs the model over-fits the training set with almost 100% accuracy, and gets about 95% accuracy on the validation set.&lt;/p&gt;

&lt;p&gt;To speed up the training, I froze the weights of the VGG16 network (in Keras this is as simple as model.trainable=False), and trained only the weights applied on the GAP layer.
Since we discarded all the layers after the last convolutional layer in VGG16, we can load a much smaller model:
https://github.com/awentzonline/keras-vgg-buddy&lt;/p&gt;

&lt;p&gt;Here are some more examples, using the weights for the “person” category:&lt;/p&gt;

&lt;p&gt;In this image it’s disappointing that the person classifier made a correct decision without even using the face regions at all.
Perhaps it should be trained on more images with clear faces.
Class activation maps look useful for understanding issues like this.
&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/keras-cam/master/examples/debate.jpg&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/keras-cam/master/examples/dog.jpg&quot; alt=&quot;enter image description here&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/keras-cam/master/examples/soccer.jpg&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s an example with weights from the “not person” category.
It looks like it’s using large “line-like” regions for making a “not person” decision.
&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/keras-cam/master/examples/traffic.jpg&quot; alt=&quot;enter image description here&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gradient-based-class-activation-maps&quot;&gt;Gradient based class activation maps&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/jacobgil/keras-grad-cam/raw/master/examples/boat.jpg?raw=true&quot; alt=&quot;grad-cam&quot; /&gt;
&lt;img src=&quot;https://github.com/jacobgil/keras-grad-cam/raw/master/examples/persian_cat.jpg?raw=true&quot; alt=&quot;grad-cam&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The original CAM method described above requires changing the network structure and then retraining it.
&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;&gt;This work&lt;/a&gt; generelizes CAM to be able to apply it with existing networks.
In case the network already has a CAM-compibtable structure, grad-cam converges to CAM.&lt;/p&gt;

&lt;h2 id=&quot;grad-cam-inputs&quot;&gt;Grad-CAM inputs:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;A query image&lt;/li&gt;
  &lt;li&gt;A network&lt;/li&gt;
  &lt;li&gt;A target function that should be maximized by pixels of interest.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;output&quot;&gt;Output:&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;A heatmap for every convolutional layer output.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Create a target function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The output of grad-cam will be pixels that contribute to the maximization of this target function. 
If for example you are interested in what maximizes category number 20, then zero out all the other categories.&lt;/p&gt;

&lt;p&gt;Simple tensorflow code that does this can look like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;target_category_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;category_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;category_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Compute the gradients of the target function, with respect to the convolutional layer outputs.
This can be done effeciently with backpropagation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Keras makes this quite easily to obtain, using the backend module.
Python code for this can look like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;    &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gradient_function&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
        &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gradient_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Use the convolutional layer output gradient image to create an importance map.
The paper does this by taking the spatial average of each channel of the gradient image, and then scaling the corresponding channel of the convolutional layer output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Instead of scaling by the spatial average like in the paper, multiplying the gradient images by the conv output images seems more natural to me, since then we get a relevance coeffecient for each pixel in each channel.&lt;/p&gt;

&lt;p&gt;We can then sum all the scaled channels to obtain the a heatmap.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Activate ReLU on the heatmap.
This keeps only pixels that have a positive influence on the target function.
The output pixels are already positive (since they come after a ReLU), so if the CAM pixel was negative, this means that there were large/many negative gradients for this pixel.&lt;/li&gt;
&lt;/ul&gt;</content><summary type="html">Github project for class activation maps
Github repo for gradient based class activation maps</summary></entry><entry><title type="html">A few notes on using the Tensorflow C++ API</title><link href="http://localhost:4000/deeplearning/tensorflow-cpp" rel="alternate" type="text/html" title="A few notes on using the Tensorflow C++ API" /><published>2016-06-10T23:10:33+03:00</published><updated>2016-06-10T23:10:33+03:00</updated><id>http://localhost:4000/deeplearning/tensorflow-with-cpp</id><content type="html" xml:base="http://localhost:4000/deeplearning/tensorflow-cpp">&lt;!--more--&gt;
&lt;p&gt;If you are unfamiliar with bazel, then there are some quirks in getting TensorFlow to work with OpenCV, optimizations turned on, and with building shared libraries.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-binary-compiled-against-tensorflow-with-bazel&quot;&gt;Creating a binary compiled against Tensorflow with bazel&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Clone the tensorflow repository.&lt;/li&gt;
  &lt;li&gt;Inside tensorflow/tensorflow, create a working directory.&lt;/li&gt;
  &lt;li&gt;Add your C++ code that uses tensorflow, lets put that in code.cpp.&lt;/li&gt;
  &lt;li&gt;Create a file called BUILD that looks like this:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt; cc_binary&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
    name &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;project&quot;&lt;/span&gt;,
    srcs &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;code.cpp&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,
    linkopts &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
		&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-lopencv_core&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;-lopencv_imgproc&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;-lopencv_highgui&quot;&lt;/span&gt;, 
		&lt;span class=&quot;s2&quot;&gt;&quot;-Wl,--version-script=tensorflow/tf_version_script.lds&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,
    copts &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-I/usr/local/include/&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;-O3&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,
    deps &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;//tensorflow/core:tensorflow&quot;&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;To build (with optimizations turned on): &lt;code class=&quot;highlighter-rouge&quot;&gt;bazel build -c opt :project&lt;/code&gt;. 
The binary will be in bazel-bin/tensorflow/project.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;creating-a-shared-library-compiled-against-tensorflow-with-bazel&quot;&gt;Creating a shared library compiled against Tensorflow with bazel&lt;/h1&gt;
&lt;p&gt;Here we want to build a shared library with C++ code that uses the Tensorflow C++ API.
This will probably be the common case for production use, since you will have a large code base with its own build system (like CMake), but you need to call Tensorflow.
Building against Tensorflow restricts you to bazel (at least that seems the simplest way for now), but you can create a shared library that can be called from the larger code base.&lt;/p&gt;

&lt;p&gt;The main issue is that bazel outputs a shared library containing only Tensorflow symbols (checked with nm -g), and *.o object files with the C++ files compiled.
That is kind of weird behaviour and seems to be an issue with bazel.
We will deal with that by just compiling against both files (the actual dynamic loaded shared library will have the tensorflow part, and our C++ client code in the object file will be linked statically. You can also just wrap the object file in another shared library).&lt;/p&gt;

&lt;p&gt;The BUILD file should now look like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bazel&quot; data-lang=&quot;bazel&quot;&gt;`cc_binary(
    name = &quot;libproject.so&quot;,
    srcs = [&quot;code.cpp&quot;],
	linkopts = [&quot;-shared&quot;, &quot;-lopencv_core&quot;,&quot;-lopencv_imgproc&quot;, &quot;-lopencv_highgui&quot;, &quot;-Wl,--version-script=tensorflow/tf_version_script.lds&quot;],
	linkshared=1,
	copts = [&quot;-I/usr/local/include/&quot;, &quot;-O3&quot;],
    deps = [
        &quot;//tensorflow/core:tensorflow&quot;
    ],
    visibility=[&quot;//visibility:public&quot;]`&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;To build: &lt;code class=&quot;highlighter-rouge&quot;&gt;bazel build -c opt --copt=&quot;-fPIC&quot; :libproject.so&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Now in bazel-bin we will have both libproject_name.so (containing only Tensorflow symbols), and the object files with our client code (under _objs/libproject.so/tensorflow/code/code.o.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now you can compile against both files, for example like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;g++ -o main main.cc tensorflow/bazel-bin/tensorflow/project/_objs/libproject.so/tensorflow/project/code.o pkg-config --libs opencv -lproject -L tensorflow/bazel-bin/tensorflow/project&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><summary type="html"></summary></entry><entry><title type="html">Visualizing CNN filters with keras</title><link href="http://localhost:4000/deeplearning/filter-visualizations" rel="alternate" type="text/html" title="Visualizing CNN filters with keras" /><published>2016-03-23T22:10:33+02:00</published><updated>2016-03-23T22:10:33+02:00</updated><id>http://localhost:4000/deeplearning/visualizing-cnn-filters-with-keras</id><content type="html" xml:base="http://localhost:4000/deeplearning/filter-visualizations">&lt;p&gt;&lt;a href=&quot;https://github.com/jacobgil/keras-filter-visualization&quot;&gt;Here&lt;/a&gt; is a utility I made for visualizing filters with Keras, using a few regularizations for more natural outputs.
&lt;!--more--&gt;
You can use it to visualize filters, and inspect the filters as they are computed.&lt;/p&gt;

&lt;p&gt;By default the utility uses the VGG16 model, but you can change that to something else.&lt;br /&gt;
The entire VGG16 model weights about 500mb.&lt;br /&gt;
However we don’t need to load the entire model if we only want to explore the the convolution filters and ignore the final fully connected layers.&lt;br /&gt;
You can download a much smaller model containing only the convolution layers (~50mb) from here:&lt;br /&gt;
&lt;a href=&quot;https://github.com/awentzonline/keras-vgg-buddy&quot;&gt;https://github.com/awentzonline/keras-vgg-buddy&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There is a lot of work being done about visualizing what deep learning networks learned.&lt;br /&gt;
This in part is due to criticism saying that it’s hard to understand what these black box networks learned, but this is also very useful to debug them.&lt;br /&gt;
Many techniques propagating gradients back to the input image became popular lately, like Google’s deep dream, or even the neural artistic style algorithm.&lt;br /&gt;
I found the Stanford cs231n course section to be good starting point for all this:&lt;br /&gt;
&lt;a href=&quot;http://cs231n.github.io/understanding-cnn/&quot;&gt;http://cs231n.github.io/understanding-cnn/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This awesome Keras blog post is a very good start for visualizing filters with Keras:&lt;br /&gt;
&lt;a href=&quot;http://blog.keras.io/how-convolutional-neural-networks-see-the-world.html&quot;&gt;http://blog.keras.io/how-convolutional-neural-networks-see-the-world.html&lt;/a&gt;&lt;br /&gt;
The idea is quite simple: we want to find an input image that would produce the largest output from one of convolution filters in one of the layers.&lt;br /&gt;
To do that, we can perform back propagation from the output of the filter we’re interested in, back to an input image. That gives us the gradient of the output of the filter with respect to the input image pixels.&lt;br /&gt;
We can use that to perform gradient ascent, searching for the image pixels that maximize the output of the filter.&lt;br /&gt;
The output of the filter is an image. We need to define a scalar score function for computing the gradient of it with respect to the image.&lt;br /&gt;
One easy way of doing that, is just taking the average output of that filter.&lt;/p&gt;

&lt;p&gt;If you look at the filters there, some look kind of noisy.&lt;br /&gt;
&lt;a href=&quot;http://yosinski.com/deepvis&quot;&gt;This project&lt;/a&gt; suggested using a combination of a few different regularizations for producing more nice looking visualizations, and I wanted to try those out.&lt;/p&gt;

&lt;h2 id=&quot;no-regularization&quot;&gt;No regularization&lt;/h2&gt;

&lt;p&gt;Lets first look at the visualization produced with gradient ascent for a few filters from the conv5_1 layer, without any regularizations:&lt;br /&gt;
&lt;img src=&quot;https://github.com/jacobgil/keras-filter-visualization/blob/master/examples/4x4_no_regularization.png?raw=true&quot; alt=&quot;4x4 with no regularization&quot; /&gt;&lt;br /&gt;
Some of the filters did not converge at all, and some have interesting patterns but are a bit noisy.&lt;/p&gt;

&lt;h2 id=&quot;l2-decay&quot;&gt;L2 decay&lt;/h2&gt;

&lt;p&gt;The first simple regularization they used in “Understanding Neural Networks Through Deep Visualization” is L2 decay.&lt;br /&gt;
The calculated image pixels are just multiplied by a constant &amp;lt; 1. This penalizes large values.&lt;br /&gt;
Here are the same filters again, using only L2 decay, multiplying the image pixels by 0.8:&lt;br /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/jacobgil/keras-filter-visualization/master/examples/4x4_decay.png&quot; alt=&quot;4x4 L2 decay regularization&quot; /&gt;&lt;br /&gt;
Notice how some of the filters contain more information, and a few of filters that previously did not converge now do.&lt;/p&gt;

&lt;h2 id=&quot;gaussian-blur&quot;&gt;Gaussian Blur&lt;/h2&gt;

&lt;p&gt;The next regularization just smooths the image with a gaussian blur.&lt;br /&gt;
In the paper above they apply it only once every few gradient ascent iterations, but here we apply it every iterations.&lt;br /&gt;
Here are the same filters, now using only gaussian blur with a 3x3 kernel:&lt;br /&gt;
&lt;img src=&quot;https://github.com/jacobgil/keras-filter-visualization/blob/master/examples/4x4_small_blur.png?raw=true&quot; alt=&quot;4x4 gaussian blur&quot; /&gt;&lt;br /&gt;
Notice how the structures become thicker, while the rest becomes smoother.&lt;/p&gt;

&lt;h2 id=&quot;removing-pixels-with-small-norms&quot;&gt;Removing pixels with small norms&lt;/h2&gt;

&lt;p&gt;This regularization zeros pixels that had weak gradient norms.&lt;br /&gt;
For each RGB channels the percentile of the average gradient value is&lt;br /&gt;
Even where a pattern doesn’t appear in the filters, pixels will have noisy non zero values.&lt;br /&gt;
By clipping weak gradients we can have more sparse outputs.&lt;/p&gt;

&lt;h2 id=&quot;gallery&quot;&gt;Gallery&lt;/h2&gt;

&lt;p&gt;Here are 256 filters with the Gaussian blur and L2 decay regularizations, and a small weight for the small norm regularization:&lt;br /&gt;
&lt;img src=&quot;https://github.com/jacobgil/keras-filter-visualization/blob/master/examples/16x16.png?raw=true&quot; alt=&quot;16x16&quot; /&gt;&lt;/p&gt;</content><summary type="html">Here is a utility I made for visualizing filters with Keras, using a few regularizations for more natural outputs.</summary></entry></feed>
